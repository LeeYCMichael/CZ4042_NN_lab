{"cells":[{"cell_type":"markdown","source":["## Tutorial 10, Question 3 (Deep Stacked Classifier)"],"metadata":{"id":"Wpvl3buWbsYS"}},{"cell_type":"code","source":["import os\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","if not os.path.isdir('figures'):\n","    print('creating the figures folder')\n","    os.makedirs('figures')"],"metadata":{"id":"b8Zb9c20iuob","executionInfo":{"status":"ok","timestamp":1699172954431,"user_tz":-480,"elapsed":322,"user":{"displayName":"Chen Change Loy","userId":"05499521047689607651"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["## Autoencoder with Tied Weights\n","\n","In this section, we define a class `Autoencoder` using PyTorch's `nn.Module` to implement an autoencoder with tied weights. The autoencoder is a type of neural network used to learn efficient representations of the input data, typically for the purpose of dimensionality reduction or feature learning.\n","\n","### Architecture Details:\n","\n","- **Initialization Method:**\n","  The `__init__` method initializes the autoencoder with the following layers and parameters:\n","\n","  - **Encoder**:\n","    - `W1`: The weight matrix for the first hidden layer in the encoder, with a size of `(n_hidden1, n_in)`. It is initialized with random values scaled by `sqrt(1/n_in)` for Xavier initialization.\n","    - `b1`: The bias vector for the first hidden layer in the encoder, initialized to zeros.\n","\n","  - **Decoder**:\n","    - `b1_prime`: The bias vector for reconstructing the input from the first hidden layer's representation, initialized to zeros. Note that the weight matrix `W1` is tied to its transpose for the decoding process.\n","\n","  - **Second Hidden Layer**:\n","    - `W2`: The weight matrix for the second hidden layer, with a size of `(n_hidden2, n_hidden1)`, initialized similarly to `W1`.\n","    - `b2`: The bias vector for the second hidden layer, initialized to zeros.\n","    - `b2_prime`: The bias vector for reconstructing the first hidden layer from the second hidden layer's representation, initialized to zeros.\n","\n","  - **Classifier**:\n","    - `W3`: The weight matrix for the output layer, with a size of `(n_out, n_hidden2)`, initialized using the same strategy as `W1` and `W2`.\n","    - `b3`: The bias vector for the output layer, initialized to zeros.\n","\n","### Forward Pass:\n","\n","The `forward` method defines the forward pass of the autoencoder:\n","\n","- The input `x` is transformed through the encoder to produce the first hidden layer representation `h1`.\n","- The representation `h1` is then used to reconstruct the input `y1` using the transpose of `W1` (implementing tied weights).\n","- The hidden layer `h1` is further encoded to produce the second hidden layer representation `h2`.\n","- The second hidden layer `h2` is used to reconstruct `h1` in `y2` using the transpose of `W2` (again using tied weights).\n","- Finally, `h2` is transformed to the output `y3` using the weights `W3` and bias `b3`.\n","\n","The activation function used in each step is the sigmoid function, which adds non-linearity to the transformations.\n","\n","This setup allows the autoencoder to learn to compress the input data into a more compact representation and then reconstruct the input from this representation as closely as possible, while also having the ability to produce a classification output.\n","\n"],"metadata":{"id":"E5fDgaBMre1q"}},{"cell_type":"code","source":["# Define the Autoencoder architecture with tied weights\n","class Autoencoder(nn.Module):\n","    def __init__(self, n_in, n_hidden1, n_hidden2, n_out):\n","        super(Autoencoder, self).__init__()\n","        # Encoder\n","        self.W1 = nn.Parameter(torch.randn(n_hidden1, n_in) * np.sqrt(1. / n_in))\n","        self.b1 = nn.Parameter(torch.zeros(n_hidden1))\n","\n","        # Decoder\n","        self.b1_prime = nn.Parameter(torch.zeros(n_in))\n","\n","        # Second hidden layer\n","        self.W2 = nn.Parameter(torch.randn(n_hidden2, n_hidden1) * np.sqrt(1. / n_hidden1))\n","        self.b2 = nn.Parameter(torch.zeros(n_hidden2))\n","        self.b2_prime = nn.Parameter(torch.zeros(n_hidden1))\n","\n","        # Classifier\n","        self.W3 = nn.Parameter(torch.randn(n_out, n_hidden2) * np.sqrt(1. / n_hidden2))\n","        self.b3 = nn.Parameter(torch.zeros(n_out))\n","\n","    def forward(self, x):\n","        h1 = torch.sigmoid(F.linear(x, self.W1, self.b1))\n","        y1 = torch.sigmoid(F.linear(h1, self.W1.t(), self.b1_prime))\n","        h2 = torch.sigmoid(F.linear(h1, self.W2, self.b2))\n","        y2 = torch.sigmoid(F.linear(h2, self.W2.t(), self.b2_prime))\n","        y3 = torch.sigmoid(F.linear(h2, self.W3, self.b3))\n","        return h1, y1, h2, y2, y3"],"metadata":{"id":"9WqkfsqXiypP","executionInfo":{"status":"ok","timestamp":1699173412300,"user_tz":-480,"elapsed":310,"user":{"displayName":"Chen Change Loy","userId":"05499521047689607651"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["# Loss functions\n","def loss_ae(h, y, original):\n","    rho = 0.02\n","    mse_loss = torch.mean(torch.sum((original - y) ** 2, dim=1))\n","    sparse_loss = torch.sum(rho * torch.log(rho / torch.mean(h, dim=0)) + (1 - rho) * torch.log((1 - rho) / (1 - torch.mean(h, dim=0))))\n","    return mse_loss + 0.4 * sparse_loss\n","\n","def loss_class(output, target):\n","    # Cross-Entropy loss\n","    return F.cross_entropy(output, target)\n","\n","def accuracy(output, target):\n","    pred = output.argmax(dim=1, keepdim=True)\n","    return pred.eq(target.view_as(pred)).float().mean()"],"metadata":{"id":"esQD2lATi2H3","executionInfo":{"status":"ok","timestamp":1699173733841,"user_tz":-480,"elapsed":329,"user":{"displayName":"Chen Change Loy","userId":"05499521047689607651"}}},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":["## Training Function for a Stepwise Autoencoder Model\n","\n","The following function, `train`, is used to train an autoencoder model in a stepwise fashion. This approach can be used to train the autoencoder and classifier components separately.\n","\n","### Parameters:\n","\n","- `model`: This is the autoencoder model to be trained.\n","- `train_loader`: The data loader that provides batches of training data.\n","- `optimizer`: The optimization algorithm used to update the weights of the model.\n","- `step`: An integer value that indicates the current training step or phase.\n","- `device`: The device (CPU or GPU) on which the model will be trained.\n","\n","### Function Overview:\n","\n","- The function sets the model to training mode using `model.train()`.\n","- It initializes `train_loss` to keep track of the cumulative loss for the epoch.\n","- The function then iterates over the `train_loader`, fetching batches of data and corresponding targets.\n","- It moves the data and targets to the specified `device`.\n","- Depending on the value of `step`, it executes a different part of the model:\n","  - **Step 1**: Trains the encoder and the first part of the decoder, optimizing the reconstruction loss between the input data and its first stage reconstruction `y1`.\n","  - **Step 2**: Continues training by optimizing the reconstruction loss between the first hidden representation `h1` and its reconstruction `y2`.\n","  - **Step 3**: Trains the classifier part of the model, optimizing the classification loss between the final output `y3` and the target labels.\n","- For each step, the function:\n","  - Clears the gradients of the optimizer.\n","  - Computes the loss using a predefined loss function specific to each step (`loss_ae` for steps 1 and 2, `loss_class` for step 3).\n","  - Accumulates the loss to `train_loss`.\n","  - Performs backpropagation using `loss.backward()`.\n","  - Updates the model parameters using `optimizer.step()`.\n","\n","### Return Value:\n","\n","- The function returns the average training loss for the epoch, which is the cumulative `train_loss` divided by the total number of items in the dataset.\n","\n","By dividing the training process into steps, the model can first learn to encode and decode the input before focusing on the classification task, which can sometimes lead to better generalization and easier training.\n","\n"],"metadata":{"id":"njf2O5hwrtJU"}},{"cell_type":"code","source":["# Training function\n","def train(model, train_loader, optimizer, step, device):\n","    model.train()\n","    train_loss = 0\n","    for data, target in train_loader:\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        if step == 1:\n","            h1, y1, _, _, _ = model(data)\n","            loss = loss_ae(h1, y1, data)\n","            train_loss += loss.item()\n","            loss.backward()\n","            optimizer.step()\n","        elif step == 2:\n","            h1, _, h2, y2, _ = model(data)\n","            loss = loss_ae(h2, y2, h1)\n","            train_loss += loss.item()\n","            loss.backward()\n","            optimizer.step()\n","        elif step == 3:\n","            _, _, _, _, y3 = model(data)\n","            loss = loss_class(y3, target)\n","            train_loss += loss.item()\n","            loss.backward()\n","            optimizer.step()\n","    return train_loss / len(train_loader.dataset)"],"metadata":{"id":"VS4hBiMEi7N6","executionInfo":{"status":"ok","timestamp":1699173736884,"user_tz":-480,"elapsed":4,"user":{"displayName":"Chen Change Loy","userId":"05499521047689607651"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["# Test function\n","def test(model, test_loader, device):\n","    model.eval()\n","    test_acc = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            _, _, _, _, output = model(data)\n","            test_acc += accuracy(output, target).item()\n","    return test_acc / len(test_loader)"],"metadata":{"id":"W6Z8tIcCi9pp","executionInfo":{"status":"ok","timestamp":1699173739322,"user_tz":-480,"elapsed":323,"user":{"displayName":"Chen Change Loy","userId":"05499521047689607651"}}},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":["## MNIST Data Loaders with Flatten Transform\n","\n","To prepare the data loaders for the MNIST dataset, we are using the `datasets.MNIST` class provided by PyTorch's `torchvision` module. We apply a series of transforms to the data to prepare it for input into our neural network model.\n","\n","### Data Loading and Transformations:\n","\n","- **Training Dataset**:\n","  - We specify `train=True` to indicate we want the training portion of the MNIST dataset.\n","  - The `download=True` parameter tells the loader to download the data if it's not present in the specified directory (`./data`).\n","  - We then define a composition of transforms:\n","    - `transforms.ToTensor()`: Converts the PIL Image or numpy.ndarray to a float tensor and scales the image's intensity values in the range [0., 1.].\n","    - `transforms.Lambda(lambda x: torch.bernoulli(x))`: Applies a Bernoulli sampling to the input tensor, effectively binarizing the image. Each pixel will be set to 1 with a probability equal to its intensity value.\n","    - `transforms.Lambda(lambda x: x.view(-1))`: Flattens the image into a 1D tensor. The `-1` in the `view` function call infers the correct dimension for flattening.\n","\n","- **Test Dataset**:\n","  - We specify `train=False` to load the test portion of the MNIST dataset.\n","  - The transformations are similar to the training dataset, except we do not apply the Bernoulli sampling. This is typically because we want to evaluate the model on unaltered test data.\n","    - `transforms.ToTensor()`: Scales and converts the image to a tensor.\n","    - `transforms.Lambda(lambda x: x.view(-1))`: Flattens the image into a 1D tensor for consistency with the training data format.\n","\n","The resulting `train_dataset` and `test_dataset` are PyTorch Dataset objects that are ready to be wrapped by a `DataLoader` for batch processing and shuffling.\n","\n"],"metadata":{"id":"r3EgHXFisBbd"}},{"cell_type":"code","source":["# MNIST data loaders with added Flatten transform\n","train_dataset = datasets.MNIST('./data', train=True, download=True,\n","                               transform=transforms.Compose([\n","                                   transforms.ToTensor(),\n","                                   transforms.Lambda(lambda x: torch.bernoulli(x)),\n","                                   transforms.Lambda(lambda x: x.view(-1))  # Flatten the images\n","                               ]))\n","\n","test_dataset = datasets.MNIST('./data', train=False,\n","                              transform=transforms.Compose([\n","                                  transforms.ToTensor(),\n","                                  transforms.Lambda(lambda x: x.view(-1))  # Flatten the images\n","                              ]))\n","\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=True)"],"metadata":{"id":"eWxDhdL1i_rG","executionInfo":{"status":"ok","timestamp":1699173742660,"user_tz":-480,"elapsed":2,"user":{"displayName":"Chen Change Loy","userId":"05499521047689607651"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Model, optimizer and steps\n","model = Autoencoder(784, 625, 100, 10).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)"],"metadata":{"id":"1pb7DtVWjI0P","executionInfo":{"status":"ok","timestamp":1699173425920,"user_tz":-480,"elapsed":410,"user":{"displayName":"Chen Change Loy","userId":"05499521047689607651"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","num_epochs = 10\n","steps = 3\n","for step in range(1, steps + 1):\n","    for epoch in range(num_epochs):\n","        train_loss = train(model, train_loader, optimizer, step, device)\n","        print(f\"Step {step}, Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss:.4f}\")\n","\n","        if epoch % 10 == 0:\n","            test_acc = test(model, test_loader, device)\n","            print(f\"Test Accuracy: {test_acc:.4f}\")\n","\n","# Save the model checkpoint\n","torch.save(model.state_dict(), 'sparse_autoencoder.pth')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"978JKHAyitlR","executionInfo":{"status":"ok","timestamp":1699174065563,"user_tz":-480,"elapsed":311933,"user":{"displayName":"Chen Change Loy","userId":"05499521047689607651"}},"outputId":"d1c1373f-5e12-4919-cdca-c215ecbfe3be"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["Step 1, Epoch [1/10], Loss: 0.9507\n","Test Accuracy: 0.0965\n","Step 1, Epoch [2/10], Loss: 0.6072\n","Step 1, Epoch [3/10], Loss: 0.5709\n","Step 1, Epoch [4/10], Loss: 0.5514\n","Step 1, Epoch [5/10], Loss: 0.5371\n","Step 1, Epoch [6/10], Loss: 0.5252\n","Step 1, Epoch [7/10], Loss: 0.5154\n","Step 1, Epoch [8/10], Loss: 0.5071\n","Step 1, Epoch [9/10], Loss: 0.4993\n","Step 1, Epoch [10/10], Loss: 0.4926\n","Step 2, Epoch [1/10], Loss: 0.2482\n","Test Accuracy: 0.1022\n","Step 2, Epoch [2/10], Loss: 0.0689\n","Step 2, Epoch [3/10], Loss: 0.0327\n","Step 2, Epoch [4/10], Loss: 0.0215\n","Step 2, Epoch [5/10], Loss: 0.0162\n","Step 2, Epoch [6/10], Loss: 0.0128\n","Step 2, Epoch [7/10], Loss: 0.0102\n","Step 2, Epoch [8/10], Loss: 0.0081\n","Step 2, Epoch [9/10], Loss: 0.0064\n","Step 2, Epoch [10/10], Loss: 0.0050\n","Step 3, Epoch [1/10], Loss: 0.0345\n","Test Accuracy: 0.7042\n","Step 3, Epoch [2/10], Loss: 0.0285\n","Step 3, Epoch [3/10], Loss: 0.0265\n","Step 3, Epoch [4/10], Loss: 0.0256\n","Step 3, Epoch [5/10], Loss: 0.0252\n","Step 3, Epoch [6/10], Loss: 0.0248\n","Step 3, Epoch [7/10], Loss: 0.0246\n","Step 3, Epoch [8/10], Loss: 0.0244\n","Step 3, Epoch [9/10], Loss: 0.0243\n","Step 3, Epoch [10/10], Loss: 0.0242\n"]}]},{"cell_type":"code","source":["# Do a final round of testing to check the accuracy\n","test_acc = test(model, test_loader, device)\n","print(f\"Test Accuracy: {test_acc:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tDdg6yRWq1yF","executionInfo":{"status":"ok","timestamp":1699174122271,"user_tz":-480,"elapsed":1422,"user":{"displayName":"Chen Change Loy","userId":"05499521047689607651"}},"outputId":"5eee4736-f732-48a2-9ba6-60b44d63f7ae"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.9285\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyNbkT3dTxN03UcMmpyQm+C5"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}