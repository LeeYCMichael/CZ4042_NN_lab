{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPWPdVLp27q+W8jJLSRf6QB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Chapter 11, Example 1 (GAN on MNIST)"],"metadata":{"id":"3__7kKkAFmoi"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import torchvision.utils as vutils\n","import os\n","import matplotlib.pyplot as plt\n","import numpy as np"],"metadata":{"id":"Ms7utVljFmGr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# MNIST Dataset\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,)),\n","])\n","\n","train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)"],"metadata":{"id":"ubCDxNULFt9D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Generator and Discriminator Classes for GAN\n","\n","### Generator\n","The `Generator` class is a neural network for generating synthetic data that mimics some distribution of real data. It takes a random noise vector as input and outputs data that should be indistinguishable from real data to the `Discriminator`.\n","\n","- `noise_dim`: The dimension of the random noise vector.\n","- The network consists of four linear layers with ReLU activations, except for the last layer which uses a Tanh activation. This is because the output is expected to be an image with pixel values normalized between -1 and 1, which is the standard for MNIST dataset.\n","- The output is reshaped into the size of an MNIST image (1 x 28 x 28).\n","\n","### Discriminator\n","The `Discriminator` class is a neural network that classifies data as real or synthetic. It takes an image as input and outputs a probability that the image is real.\n","\n","- The input images are first flattened.\n","- The network consists of four linear layers with ReLU activations, leading to a single output through a Sigmoid activation function. The Sigmoid is used to output a probability between 0 and 1, indicating how likely it is that the input data is from the real dataset as opposed to being generated by the `Generator`.\n","\n","### Forward Methods\n","Both classes have a `forward` method that defines how the data flows through the network:\n","- For the `Generator`, the `forward` method takes a noise vector `z`, passes it through the network, and reshapes the output to the size of an MNIST image.\n","- For the `Discriminator`, the `forward` method takes an image `x`, flattens it, and passes it through the network to obtain the probability of the image being real.\n","\n","These classes are essential components of a Generative Adversarial Network (GAN), where the `Generator` learns to produce more realistic images over time, and the `Discriminator` learns to better distinguish between real and generated images.\n"],"metadata":{"id":"hqAxie7VHNh4"}},{"cell_type":"code","source":["# Generator\n","class Generator(nn.Module):\n","    def __init__(self, noise_dim=100):\n","        super(Generator, self).__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(noise_dim, 256),\n","            nn.ReLU(True),\n","            nn.Linear(256, 512),\n","            nn.ReLU(True),\n","            nn.Linear(512, 1024),\n","            nn.ReLU(True),\n","            nn.Linear(1024, 28*28),\n","            nn.Tanh()  # Use Tanh for the last layer because MNIST pixel values are between -1 and 1\n","        )\n","\n","    def forward(self, z):\n","        return self.net(z).view(-1, 1, 28, 28)\n","\n","# Discriminator\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(28*28, 1024),\n","            nn.ReLU(True),\n","            nn.Linear(1024, 512),\n","            nn.ReLU(True),\n","            nn.Linear(512, 256),\n","            nn.ReLU(True),\n","            nn.Linear(256, 1),\n","            nn.Sigmoid()  # Sigmoid activation because we need probability outputs\n","        )\n","\n","    def forward(self, x):\n","        x = x.view(-1, 28*28)\n","        return self.net(x)"],"metadata":{"id":"nsr7lkNYFwsu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize Generator and Discriminator\n","generator = Generator().to(device)\n","discriminator = Discriminator().to(device)\n","\n","# Loss and optimizer\n","criterion = nn.BCELoss()\n","optimizer_g = optim.Adam(generator.parameters(), lr=0.0002)\n","optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002)\n","\n","# Noise sampler\n","def sample_noise(batch_size, noise_dim):\n","    return torch.rand(batch_size, noise_dim, device=device)*2 - 1  # Uniform noise [-1, 1]"],"metadata":{"id":"3wBqHYvJF4YG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training Loop for GAN\n","\n","The training process involves alternating between updating the `Discriminator` and the `Generator`. Here's a breakdown of the training loop:\n","\n","### Parameters:\n","- `num_epochs`: The number of times to iterate over the entire dataset.\n","- `noise_dim`: The dimensionality of the noise vector fed into the `Generator`.\n","\n","### Training Loop:\n","For each epoch in `num_epochs`:\n","1. Iterate over the `train_loader`, which provides batches of real images and their associated labels (which are not used here).\n","2. For each batch:\n","   - The `batch_size` is determined based on the number of images in the current batch.\n","   - `real_labels` are tensor of ones, and `fake_labels` are tensor of zeros, representing the true labels for real and fake data, respectively.\n","\n","### Training the Discriminator:\n","- Zero the gradients of the `Discriminator` before the forward pass.\n","- Compute the `Discriminator`'s loss on real images (`loss_d_real`) and perform a backward pass to calculate gradients.\n","- Generate fake images using the `Generator` with random noise `z`.\n","- Compute the `Discriminator`'s loss on the fake images (`loss_d_fake`) and perform a backward pass to calculate gradients.\n","- The total loss for the `Discriminator` is the sum of `loss_d_real` and `loss_d_fake`.\n","- Update the `Discriminator`'s weights with a step of the optimizer (`optimizer_d`).\n","\n","### Training the Generator:\n","- Zero the gradients of the `Generator` before the forward pass.\n","- Pass the fake images through the `Discriminator` and calculate the loss (`loss_g`) using real labels, to encourage the `Generator` to produce better fakes.\n","- Perform a backward pass to calculate gradients for the `Generator`.\n","- Update the `Generator`'s weights with a step of the optimizer (`optimizer_g`).\n","\n","This training loop is designed to improve the `Generator`'s ability to create images that are indistinguishable from real ones, while the `Discriminator` becomes better at telling them apart. The `Discriminator` is trained first within each batch, followed by the `Generator`, ensuring that the `Generator` uses the most up-to-date version of the `Discriminator` for its updates.\n"],"metadata":{"id":"tHfWt5svH1RX"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kG52wXf6Dd_k","executionInfo":{"status":"ok","timestamp":1699248673647,"user_tz":-480,"elapsed":925178,"user":{"displayName":"Chen Change Loy","userId":"05499521047689607651"}},"outputId":"dca6a5c0-71a1-491d-d3c7-290ab4166cfb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 101804937.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 103358100.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 61235092.94it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 5576852.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n","\n","Epoch [1/50], Step [100/938], Loss D: 0.4443, Loss G: 1.4049\n","Epoch [1/50], Step [200/938], Loss D: 0.0327, Loss G: 4.2727\n","Epoch [1/50], Step [300/938], Loss D: 0.0051, Loss G: 26.2855\n","Epoch [1/50], Step [400/938], Loss D: 0.3268, Loss G: 5.2529\n","Epoch [1/50], Step [500/938], Loss D: 1.2156, Loss G: 4.4732\n","Epoch [1/50], Step [600/938], Loss D: 2.8881, Loss G: 3.4738\n","Epoch [1/50], Step [700/938], Loss D: 0.3673, Loss G: 5.5105\n","Epoch [1/50], Step [800/938], Loss D: 0.7306, Loss G: 2.2787\n","Epoch [1/50], Step [900/938], Loss D: 0.5731, Loss G: 1.5423\n","Epoch [2/50], Step [100/938], Loss D: 0.2681, Loss G: 2.7369\n","Epoch [2/50], Step [200/938], Loss D: 0.3621, Loss G: 2.7035\n","Epoch [2/50], Step [300/938], Loss D: 0.8924, Loss G: 1.6316\n","Epoch [2/50], Step [400/938], Loss D: 0.3813, Loss G: 2.9162\n","Epoch [2/50], Step [500/938], Loss D: 0.2133, Loss G: 3.6234\n","Epoch [2/50], Step [600/938], Loss D: 2.0276, Loss G: 4.6747\n","Epoch [2/50], Step [700/938], Loss D: 0.6046, Loss G: 2.8962\n","Epoch [2/50], Step [800/938], Loss D: 0.1181, Loss G: 5.2332\n","Epoch [2/50], Step [900/938], Loss D: 0.9689, Loss G: 2.6647\n","Epoch [3/50], Step [100/938], Loss D: 1.0359, Loss G: 3.1839\n","Epoch [3/50], Step [200/938], Loss D: 0.2474, Loss G: 2.9517\n","Epoch [3/50], Step [300/938], Loss D: 0.2221, Loss G: 4.8225\n","Epoch [3/50], Step [400/938], Loss D: 0.3738, Loss G: 4.8405\n","Epoch [3/50], Step [500/938], Loss D: 0.2269, Loss G: 5.4979\n","Epoch [3/50], Step [600/938], Loss D: 0.8850, Loss G: 2.5109\n","Epoch [3/50], Step [700/938], Loss D: 0.1546, Loss G: 7.0471\n","Epoch [3/50], Step [800/938], Loss D: 0.2305, Loss G: 4.7901\n","Epoch [3/50], Step [900/938], Loss D: 0.1995, Loss G: 2.5718\n","Epoch [4/50], Step [100/938], Loss D: 0.5909, Loss G: 5.6655\n","Epoch [4/50], Step [200/938], Loss D: 0.3013, Loss G: 3.5994\n","Epoch [4/50], Step [300/938], Loss D: 0.1434, Loss G: 5.3928\n","Epoch [4/50], Step [400/938], Loss D: 0.1346, Loss G: 5.9795\n","Epoch [4/50], Step [500/938], Loss D: 0.0948, Loss G: 5.2065\n","Epoch [4/50], Step [600/938], Loss D: 0.4792, Loss G: 4.0928\n","Epoch [4/50], Step [700/938], Loss D: 0.4820, Loss G: 3.7674\n","Epoch [4/50], Step [800/938], Loss D: 0.0310, Loss G: 7.5861\n","Epoch [4/50], Step [900/938], Loss D: 0.0251, Loss G: 6.7925\n","Epoch [5/50], Step [100/938], Loss D: 0.2245, Loss G: 4.1950\n","Epoch [5/50], Step [200/938], Loss D: 0.3772, Loss G: 7.7612\n","Epoch [5/50], Step [300/938], Loss D: 0.1297, Loss G: 4.8365\n","Epoch [5/50], Step [400/938], Loss D: 0.2798, Loss G: 5.2299\n","Epoch [5/50], Step [500/938], Loss D: 0.3060, Loss G: 5.4496\n","Epoch [5/50], Step [600/938], Loss D: 0.1374, Loss G: 6.1209\n","Epoch [5/50], Step [700/938], Loss D: 0.4837, Loss G: 3.6761\n","Epoch [5/50], Step [800/938], Loss D: 0.0961, Loss G: 5.8527\n","Epoch [5/50], Step [900/938], Loss D: 0.4914, Loss G: 5.7060\n","Epoch [6/50], Step [100/938], Loss D: 0.3555, Loss G: 4.9189\n","Epoch [6/50], Step [200/938], Loss D: 0.4519, Loss G: 3.1580\n","Epoch [6/50], Step [300/938], Loss D: 0.2693, Loss G: 3.9708\n","Epoch [6/50], Step [400/938], Loss D: 0.2479, Loss G: 8.2612\n","Epoch [6/50], Step [500/938], Loss D: 0.4856, Loss G: 3.5799\n","Epoch [6/50], Step [600/938], Loss D: 0.1722, Loss G: 3.7287\n","Epoch [6/50], Step [700/938], Loss D: 0.1110, Loss G: 5.7281\n","Epoch [6/50], Step [800/938], Loss D: 0.6498, Loss G: 4.9477\n","Epoch [6/50], Step [900/938], Loss D: 0.1363, Loss G: 5.0439\n","Epoch [7/50], Step [100/938], Loss D: 0.0679, Loss G: 4.5835\n","Epoch [7/50], Step [200/938], Loss D: 0.5697, Loss G: 6.7629\n","Epoch [7/50], Step [300/938], Loss D: 0.0979, Loss G: 4.1230\n","Epoch [7/50], Step [400/938], Loss D: 0.1764, Loss G: 2.7895\n","Epoch [7/50], Step [500/938], Loss D: 0.2827, Loss G: 4.2366\n","Epoch [7/50], Step [600/938], Loss D: 0.1970, Loss G: 6.2301\n","Epoch [7/50], Step [700/938], Loss D: 0.0397, Loss G: 4.4866\n","Epoch [7/50], Step [800/938], Loss D: 0.3376, Loss G: 6.8802\n","Epoch [7/50], Step [900/938], Loss D: 0.1179, Loss G: 5.8583\n","Epoch [8/50], Step [100/938], Loss D: 0.2811, Loss G: 6.7228\n","Epoch [8/50], Step [200/938], Loss D: 0.5122, Loss G: 4.7350\n","Epoch [8/50], Step [300/938], Loss D: 0.3709, Loss G: 3.4978\n","Epoch [8/50], Step [400/938], Loss D: 0.3652, Loss G: 3.7908\n","Epoch [8/50], Step [500/938], Loss D: 0.1468, Loss G: 4.1674\n","Epoch [8/50], Step [600/938], Loss D: 0.3209, Loss G: 6.1625\n","Epoch [8/50], Step [700/938], Loss D: 0.3852, Loss G: 7.7227\n","Epoch [8/50], Step [800/938], Loss D: 0.0509, Loss G: 6.4799\n","Epoch [8/50], Step [900/938], Loss D: 0.2543, Loss G: 4.5274\n","Epoch [9/50], Step [100/938], Loss D: 0.2778, Loss G: 3.9836\n","Epoch [9/50], Step [200/938], Loss D: 0.5742, Loss G: 4.6170\n","Epoch [9/50], Step [300/938], Loss D: 0.6560, Loss G: 2.1204\n","Epoch [9/50], Step [400/938], Loss D: 0.4434, Loss G: 3.0787\n","Epoch [9/50], Step [500/938], Loss D: 0.3241, Loss G: 4.1042\n","Epoch [9/50], Step [600/938], Loss D: 0.1509, Loss G: 4.0630\n","Epoch [9/50], Step [700/938], Loss D: 0.2095, Loss G: 5.7506\n","Epoch [9/50], Step [800/938], Loss D: 0.4394, Loss G: 5.8906\n","Epoch [9/50], Step [900/938], Loss D: 0.2126, Loss G: 4.1518\n","Epoch [10/50], Step [100/938], Loss D: 0.2265, Loss G: 4.6861\n","Epoch [10/50], Step [200/938], Loss D: 0.2679, Loss G: 4.4225\n","Epoch [10/50], Step [300/938], Loss D: 0.5090, Loss G: 5.9025\n","Epoch [10/50], Step [400/938], Loss D: 0.2200, Loss G: 6.6742\n","Epoch [10/50], Step [500/938], Loss D: 0.5220, Loss G: 2.9242\n","Epoch [10/50], Step [600/938], Loss D: 0.1640, Loss G: 3.6223\n","Epoch [10/50], Step [700/938], Loss D: 0.2790, Loss G: 4.2515\n","Epoch [10/50], Step [800/938], Loss D: 0.5669, Loss G: 4.3229\n","Epoch [10/50], Step [900/938], Loss D: 0.2555, Loss G: 4.3736\n","Epoch [11/50], Step [100/938], Loss D: 0.0951, Loss G: 4.0584\n","Epoch [11/50], Step [200/938], Loss D: 0.3302, Loss G: 4.7393\n","Epoch [11/50], Step [300/938], Loss D: 0.3815, Loss G: 3.9873\n","Epoch [11/50], Step [400/938], Loss D: 0.0183, Loss G: 11.4648\n","Epoch [11/50], Step [500/938], Loss D: 0.6071, Loss G: 3.3844\n","Epoch [11/50], Step [600/938], Loss D: 0.4283, Loss G: 3.7904\n","Epoch [11/50], Step [700/938], Loss D: 0.3935, Loss G: 5.9266\n","Epoch [11/50], Step [800/938], Loss D: 0.3256, Loss G: 5.2250\n","Epoch [11/50], Step [900/938], Loss D: 0.1851, Loss G: 4.2328\n","Epoch [12/50], Step [100/938], Loss D: 0.2598, Loss G: 5.5759\n","Epoch [12/50], Step [200/938], Loss D: 0.1755, Loss G: 5.2623\n","Epoch [12/50], Step [300/938], Loss D: 0.1599, Loss G: 4.4276\n","Epoch [12/50], Step [400/938], Loss D: 0.4160, Loss G: 4.0199\n","Epoch [12/50], Step [500/938], Loss D: 0.1420, Loss G: 4.8718\n","Epoch [12/50], Step [600/938], Loss D: 0.2047, Loss G: 5.2414\n","Epoch [12/50], Step [700/938], Loss D: 0.5406, Loss G: 4.3168\n","Epoch [12/50], Step [800/938], Loss D: 0.3809, Loss G: 5.3561\n","Epoch [12/50], Step [900/938], Loss D: 0.3896, Loss G: 4.7942\n","Epoch [13/50], Step [100/938], Loss D: 0.2434, Loss G: 4.8001\n","Epoch [13/50], Step [200/938], Loss D: 0.1956, Loss G: 4.1820\n","Epoch [13/50], Step [300/938], Loss D: 0.1897, Loss G: 3.5524\n","Epoch [13/50], Step [400/938], Loss D: 0.1775, Loss G: 5.0700\n","Epoch [13/50], Step [500/938], Loss D: 0.2184, Loss G: 4.2848\n","Epoch [13/50], Step [600/938], Loss D: 0.1517, Loss G: 4.3998\n","Epoch [13/50], Step [700/938], Loss D: 0.2870, Loss G: 6.0880\n","Epoch [13/50], Step [800/938], Loss D: 0.3186, Loss G: 3.4606\n","Epoch [13/50], Step [900/938], Loss D: 0.2806, Loss G: 4.5548\n","Epoch [14/50], Step [100/938], Loss D: 0.3685, Loss G: 6.0050\n","Epoch [14/50], Step [200/938], Loss D: 0.2989, Loss G: 3.5141\n","Epoch [14/50], Step [300/938], Loss D: 0.5166, Loss G: 3.1114\n","Epoch [14/50], Step [400/938], Loss D: 0.2590, Loss G: 3.8197\n","Epoch [14/50], Step [500/938], Loss D: 0.2887, Loss G: 2.4128\n","Epoch [14/50], Step [600/938], Loss D: 0.3053, Loss G: 2.2358\n","Epoch [14/50], Step [700/938], Loss D: 0.4881, Loss G: 3.1868\n","Epoch [14/50], Step [800/938], Loss D: 0.1857, Loss G: 4.6976\n","Epoch [14/50], Step [900/938], Loss D: 0.3179, Loss G: 3.6939\n","Epoch [15/50], Step [100/938], Loss D: 0.3076, Loss G: 4.9397\n","Epoch [15/50], Step [200/938], Loss D: 0.1882, Loss G: 4.0911\n","Epoch [15/50], Step [300/938], Loss D: 0.3169, Loss G: 4.9755\n","Epoch [15/50], Step [400/938], Loss D: 0.1886, Loss G: 3.9745\n","Epoch [15/50], Step [500/938], Loss D: 0.2819, Loss G: 3.9376\n","Epoch [15/50], Step [600/938], Loss D: 0.3571, Loss G: 4.7606\n","Epoch [15/50], Step [700/938], Loss D: 0.2676, Loss G: 4.5974\n","Epoch [15/50], Step [800/938], Loss D: 0.2425, Loss G: 3.2635\n","Epoch [15/50], Step [900/938], Loss D: 0.2652, Loss G: 5.1334\n","Epoch [16/50], Step [100/938], Loss D: 0.3395, Loss G: 3.5165\n","Epoch [16/50], Step [200/938], Loss D: 0.2718, Loss G: 2.6521\n","Epoch [16/50], Step [300/938], Loss D: 0.3323, Loss G: 4.1085\n","Epoch [16/50], Step [400/938], Loss D: 0.1750, Loss G: 3.0828\n","Epoch [16/50], Step [500/938], Loss D: 0.3751, Loss G: 3.6458\n","Epoch [16/50], Step [600/938], Loss D: 0.3814, Loss G: 3.5829\n","Epoch [16/50], Step [700/938], Loss D: 0.4966, Loss G: 4.1465\n","Epoch [16/50], Step [800/938], Loss D: 0.2512, Loss G: 4.2251\n","Epoch [16/50], Step [900/938], Loss D: 0.2905, Loss G: 3.2039\n","Epoch [17/50], Step [100/938], Loss D: 0.2715, Loss G: 2.7701\n","Epoch [17/50], Step [200/938], Loss D: 0.3123, Loss G: 3.2946\n","Epoch [17/50], Step [300/938], Loss D: 0.2849, Loss G: 3.8913\n","Epoch [17/50], Step [400/938], Loss D: 0.2496, Loss G: 5.1287\n","Epoch [17/50], Step [500/938], Loss D: 0.4567, Loss G: 2.5058\n","Epoch [17/50], Step [600/938], Loss D: 0.6114, Loss G: 2.9101\n","Epoch [17/50], Step [700/938], Loss D: 0.2717, Loss G: 3.5251\n","Epoch [17/50], Step [800/938], Loss D: 0.3780, Loss G: 3.2590\n","Epoch [17/50], Step [900/938], Loss D: 0.4754, Loss G: 3.0175\n","Epoch [18/50], Step [100/938], Loss D: 0.4062, Loss G: 2.9016\n","Epoch [18/50], Step [200/938], Loss D: 0.2177, Loss G: 3.6525\n","Epoch [18/50], Step [300/938], Loss D: 0.1988, Loss G: 3.2364\n","Epoch [18/50], Step [400/938], Loss D: 0.4331, Loss G: 2.3206\n","Epoch [18/50], Step [500/938], Loss D: 0.3732, Loss G: 4.2548\n","Epoch [18/50], Step [600/938], Loss D: 0.2409, Loss G: 3.1533\n","Epoch [18/50], Step [700/938], Loss D: 0.3995, Loss G: 3.6256\n","Epoch [18/50], Step [800/938], Loss D: 0.5651, Loss G: 2.8710\n","Epoch [18/50], Step [900/938], Loss D: 0.4114, Loss G: 3.4395\n","Epoch [19/50], Step [100/938], Loss D: 0.3494, Loss G: 4.0613\n","Epoch [19/50], Step [200/938], Loss D: 0.2831, Loss G: 3.2105\n","Epoch [19/50], Step [300/938], Loss D: 0.2376, Loss G: 3.5391\n","Epoch [19/50], Step [400/938], Loss D: 0.3541, Loss G: 3.4324\n","Epoch [19/50], Step [500/938], Loss D: 0.3404, Loss G: 4.0488\n","Epoch [19/50], Step [600/938], Loss D: 0.3444, Loss G: 2.4757\n","Epoch [19/50], Step [700/938], Loss D: 0.2263, Loss G: 3.5285\n","Epoch [19/50], Step [800/938], Loss D: 0.4701, Loss G: 2.2033\n","Epoch [19/50], Step [900/938], Loss D: 0.5781, Loss G: 2.8288\n","Epoch [20/50], Step [100/938], Loss D: 0.5093, Loss G: 2.1863\n","Epoch [20/50], Step [200/938], Loss D: 0.3739, Loss G: 3.6289\n","Epoch [20/50], Step [300/938], Loss D: 0.5337, Loss G: 2.1979\n","Epoch [20/50], Step [400/938], Loss D: 0.4486, Loss G: 2.4987\n","Epoch [20/50], Step [500/938], Loss D: 0.4271, Loss G: 3.5376\n","Epoch [20/50], Step [600/938], Loss D: 0.2447, Loss G: 2.8849\n","Epoch [20/50], Step [700/938], Loss D: 0.4506, Loss G: 2.6857\n","Epoch [20/50], Step [800/938], Loss D: 0.5256, Loss G: 2.3421\n","Epoch [20/50], Step [900/938], Loss D: 0.4094, Loss G: 3.4902\n","Epoch [21/50], Step [100/938], Loss D: 0.7301, Loss G: 3.2369\n","Epoch [21/50], Step [200/938], Loss D: 0.6442, Loss G: 1.5031\n","Epoch [21/50], Step [300/938], Loss D: 0.5564, Loss G: 2.9728\n","Epoch [21/50], Step [400/938], Loss D: 0.5008, Loss G: 2.6931\n","Epoch [21/50], Step [500/938], Loss D: 0.6112, Loss G: 2.5625\n","Epoch [21/50], Step [600/938], Loss D: 0.6749, Loss G: 2.8208\n","Epoch [21/50], Step [700/938], Loss D: 0.3647, Loss G: 2.5143\n","Epoch [21/50], Step [800/938], Loss D: 0.4714, Loss G: 2.5566\n","Epoch [21/50], Step [900/938], Loss D: 0.5538, Loss G: 3.1341\n","Epoch [22/50], Step [100/938], Loss D: 0.6843, Loss G: 2.5668\n","Epoch [22/50], Step [200/938], Loss D: 0.4686, Loss G: 2.8495\n","Epoch [22/50], Step [300/938], Loss D: 0.6082, Loss G: 2.2397\n","Epoch [22/50], Step [400/938], Loss D: 0.5787, Loss G: 3.0874\n","Epoch [22/50], Step [500/938], Loss D: 0.4824, Loss G: 2.6489\n","Epoch [22/50], Step [600/938], Loss D: 0.6076, Loss G: 2.0111\n","Epoch [22/50], Step [700/938], Loss D: 0.6201, Loss G: 2.2576\n","Epoch [22/50], Step [800/938], Loss D: 0.4538, Loss G: 2.8218\n","Epoch [22/50], Step [900/938], Loss D: 0.6926, Loss G: 2.1611\n","Epoch [23/50], Step [100/938], Loss D: 0.8492, Loss G: 2.6247\n","Epoch [23/50], Step [200/938], Loss D: 0.6075, Loss G: 2.1041\n","Epoch [23/50], Step [300/938], Loss D: 0.6293, Loss G: 2.1168\n","Epoch [23/50], Step [400/938], Loss D: 0.5460, Loss G: 2.6779\n","Epoch [23/50], Step [500/938], Loss D: 0.5368, Loss G: 2.2750\n","Epoch [23/50], Step [600/938], Loss D: 0.6392, Loss G: 2.4735\n","Epoch [23/50], Step [700/938], Loss D: 0.6440, Loss G: 2.0217\n","Epoch [23/50], Step [800/938], Loss D: 0.5156, Loss G: 1.9538\n","Epoch [23/50], Step [900/938], Loss D: 0.6328, Loss G: 2.6026\n","Epoch [24/50], Step [100/938], Loss D: 0.5572, Loss G: 1.9043\n","Epoch [24/50], Step [200/938], Loss D: 0.7077, Loss G: 1.8022\n","Epoch [24/50], Step [300/938], Loss D: 0.5088, Loss G: 2.3138\n","Epoch [24/50], Step [400/938], Loss D: 0.7160, Loss G: 1.7365\n","Epoch [24/50], Step [500/938], Loss D: 0.7564, Loss G: 2.6165\n","Epoch [24/50], Step [600/938], Loss D: 0.5552, Loss G: 1.7337\n","Epoch [24/50], Step [700/938], Loss D: 0.6642, Loss G: 2.4571\n","Epoch [24/50], Step [800/938], Loss D: 0.6275, Loss G: 2.0294\n","Epoch [24/50], Step [900/938], Loss D: 0.5725, Loss G: 1.7636\n","Epoch [25/50], Step [100/938], Loss D: 0.6198, Loss G: 1.8693\n","Epoch [25/50], Step [200/938], Loss D: 0.7343, Loss G: 2.1499\n","Epoch [25/50], Step [300/938], Loss D: 0.4501, Loss G: 2.1304\n","Epoch [25/50], Step [400/938], Loss D: 0.4822, Loss G: 2.8658\n","Epoch [25/50], Step [500/938], Loss D: 0.4921, Loss G: 2.7609\n","Epoch [25/50], Step [600/938], Loss D: 0.5420, Loss G: 2.0266\n","Epoch [25/50], Step [700/938], Loss D: 0.8507, Loss G: 1.8991\n","Epoch [25/50], Step [800/938], Loss D: 0.5214, Loss G: 1.9790\n","Epoch [25/50], Step [900/938], Loss D: 0.6319, Loss G: 1.7724\n","Epoch [26/50], Step [100/938], Loss D: 0.5144, Loss G: 2.0820\n","Epoch [26/50], Step [200/938], Loss D: 0.5700, Loss G: 1.5962\n","Epoch [26/50], Step [300/938], Loss D: 0.5513, Loss G: 2.1172\n","Epoch [26/50], Step [400/938], Loss D: 0.5191, Loss G: 2.0462\n","Epoch [26/50], Step [500/938], Loss D: 0.6374, Loss G: 2.2385\n","Epoch [26/50], Step [600/938], Loss D: 0.6828, Loss G: 1.6806\n","Epoch [26/50], Step [700/938], Loss D: 0.8147, Loss G: 1.7675\n","Epoch [26/50], Step [800/938], Loss D: 0.6876, Loss G: 2.2126\n","Epoch [26/50], Step [900/938], Loss D: 0.8979, Loss G: 2.1312\n","Epoch [27/50], Step [100/938], Loss D: 0.6410, Loss G: 1.8167\n","Epoch [27/50], Step [200/938], Loss D: 0.7564, Loss G: 2.2695\n","Epoch [27/50], Step [300/938], Loss D: 0.8334, Loss G: 1.6218\n","Epoch [27/50], Step [400/938], Loss D: 0.8397, Loss G: 2.0658\n","Epoch [27/50], Step [500/938], Loss D: 0.7245, Loss G: 1.9565\n","Epoch [27/50], Step [600/938], Loss D: 0.7435, Loss G: 2.1715\n","Epoch [27/50], Step [700/938], Loss D: 0.6740, Loss G: 1.7725\n","Epoch [27/50], Step [800/938], Loss D: 0.7012, Loss G: 2.0242\n","Epoch [27/50], Step [900/938], Loss D: 0.7387, Loss G: 1.4956\n","Epoch [28/50], Step [100/938], Loss D: 0.6930, Loss G: 2.1825\n","Epoch [28/50], Step [200/938], Loss D: 0.7882, Loss G: 1.8114\n","Epoch [28/50], Step [300/938], Loss D: 0.7044, Loss G: 1.8251\n","Epoch [28/50], Step [400/938], Loss D: 0.6361, Loss G: 1.8978\n","Epoch [28/50], Step [500/938], Loss D: 0.8202, Loss G: 1.8432\n","Epoch [28/50], Step [600/938], Loss D: 0.5650, Loss G: 2.0304\n","Epoch [28/50], Step [700/938], Loss D: 0.5232, Loss G: 1.7907\n","Epoch [28/50], Step [800/938], Loss D: 0.9186, Loss G: 1.7373\n","Epoch [28/50], Step [900/938], Loss D: 0.7706, Loss G: 1.8421\n","Epoch [29/50], Step [100/938], Loss D: 0.7469, Loss G: 1.7490\n","Epoch [29/50], Step [200/938], Loss D: 0.6680, Loss G: 1.8676\n","Epoch [29/50], Step [300/938], Loss D: 0.6584, Loss G: 1.6719\n","Epoch [29/50], Step [400/938], Loss D: 0.5583, Loss G: 1.9377\n","Epoch [29/50], Step [500/938], Loss D: 0.7326, Loss G: 1.9769\n","Epoch [29/50], Step [600/938], Loss D: 0.5485, Loss G: 2.1288\n","Epoch [29/50], Step [700/938], Loss D: 0.5244, Loss G: 2.4939\n","Epoch [29/50], Step [800/938], Loss D: 0.9900, Loss G: 1.7282\n","Epoch [29/50], Step [900/938], Loss D: 0.6319, Loss G: 2.0617\n","Epoch [30/50], Step [100/938], Loss D: 0.7986, Loss G: 1.4101\n","Epoch [30/50], Step [200/938], Loss D: 0.6134, Loss G: 1.8162\n","Epoch [30/50], Step [300/938], Loss D: 0.6130, Loss G: 1.9647\n","Epoch [30/50], Step [400/938], Loss D: 0.8141, Loss G: 1.5487\n","Epoch [30/50], Step [500/938], Loss D: 0.6563, Loss G: 2.1962\n","Epoch [30/50], Step [600/938], Loss D: 0.8104, Loss G: 2.0262\n","Epoch [30/50], Step [700/938], Loss D: 0.7693, Loss G: 1.8281\n","Epoch [30/50], Step [800/938], Loss D: 0.7695, Loss G: 1.4940\n","Epoch [30/50], Step [900/938], Loss D: 0.7953, Loss G: 1.3511\n","Epoch [31/50], Step [100/938], Loss D: 0.9497, Loss G: 1.4894\n","Epoch [31/50], Step [200/938], Loss D: 0.8225, Loss G: 2.1434\n","Epoch [31/50], Step [300/938], Loss D: 0.8387, Loss G: 1.5431\n","Epoch [31/50], Step [400/938], Loss D: 0.5936, Loss G: 1.7935\n","Epoch [31/50], Step [500/938], Loss D: 0.8255, Loss G: 1.6598\n","Epoch [31/50], Step [600/938], Loss D: 0.8057, Loss G: 1.6940\n","Epoch [31/50], Step [700/938], Loss D: 0.8069, Loss G: 1.8106\n","Epoch [31/50], Step [800/938], Loss D: 0.5670, Loss G: 1.5145\n","Epoch [31/50], Step [900/938], Loss D: 0.6664, Loss G: 1.9941\n","Epoch [32/50], Step [100/938], Loss D: 0.7360, Loss G: 2.0690\n","Epoch [32/50], Step [200/938], Loss D: 0.8030, Loss G: 1.6135\n","Epoch [32/50], Step [300/938], Loss D: 0.8197, Loss G: 2.0549\n","Epoch [32/50], Step [400/938], Loss D: 0.6754, Loss G: 1.7522\n","Epoch [32/50], Step [500/938], Loss D: 0.7569, Loss G: 1.4528\n","Epoch [32/50], Step [600/938], Loss D: 0.6817, Loss G: 1.6705\n","Epoch [32/50], Step [700/938], Loss D: 0.7061, Loss G: 1.5765\n","Epoch [32/50], Step [800/938], Loss D: 0.8641, Loss G: 1.8063\n","Epoch [32/50], Step [900/938], Loss D: 0.7374, Loss G: 1.6891\n","Epoch [33/50], Step [100/938], Loss D: 0.7780, Loss G: 1.6480\n","Epoch [33/50], Step [200/938], Loss D: 0.7185, Loss G: 1.8313\n","Epoch [33/50], Step [300/938], Loss D: 0.8444, Loss G: 1.6887\n","Epoch [33/50], Step [400/938], Loss D: 0.8756, Loss G: 1.5696\n","Epoch [33/50], Step [500/938], Loss D: 0.7939, Loss G: 1.5036\n","Epoch [33/50], Step [600/938], Loss D: 0.8791, Loss G: 1.2044\n","Epoch [33/50], Step [700/938], Loss D: 0.6889, Loss G: 1.8555\n","Epoch [33/50], Step [800/938], Loss D: 0.7904, Loss G: 1.9648\n","Epoch [33/50], Step [900/938], Loss D: 1.0907, Loss G: 1.2429\n","Epoch [34/50], Step [100/938], Loss D: 0.8543, Loss G: 1.6403\n","Epoch [34/50], Step [200/938], Loss D: 1.0679, Loss G: 1.5487\n","Epoch [34/50], Step [300/938], Loss D: 0.6504, Loss G: 1.6490\n","Epoch [34/50], Step [400/938], Loss D: 0.6313, Loss G: 1.7092\n","Epoch [34/50], Step [500/938], Loss D: 0.8476, Loss G: 1.6626\n","Epoch [34/50], Step [600/938], Loss D: 0.8076, Loss G: 1.6973\n","Epoch [34/50], Step [700/938], Loss D: 0.7996, Loss G: 1.7964\n","Epoch [34/50], Step [800/938], Loss D: 0.7226, Loss G: 2.1516\n","Epoch [34/50], Step [900/938], Loss D: 0.9208, Loss G: 1.9753\n","Epoch [35/50], Step [100/938], Loss D: 0.7466, Loss G: 1.8374\n","Epoch [35/50], Step [200/938], Loss D: 0.7644, Loss G: 2.4730\n","Epoch [35/50], Step [300/938], Loss D: 0.6763, Loss G: 1.9563\n","Epoch [35/50], Step [400/938], Loss D: 0.7948, Loss G: 1.8108\n","Epoch [35/50], Step [500/938], Loss D: 0.8963, Loss G: 1.4384\n","Epoch [35/50], Step [600/938], Loss D: 0.7559, Loss G: 1.8867\n","Epoch [35/50], Step [700/938], Loss D: 0.6191, Loss G: 1.6044\n","Epoch [35/50], Step [800/938], Loss D: 0.9571, Loss G: 2.2486\n","Epoch [35/50], Step [900/938], Loss D: 1.0877, Loss G: 1.5642\n","Epoch [36/50], Step [100/938], Loss D: 0.6760, Loss G: 1.6760\n","Epoch [36/50], Step [200/938], Loss D: 0.6701, Loss G: 2.1811\n","Epoch [36/50], Step [300/938], Loss D: 1.0105, Loss G: 1.1371\n","Epoch [36/50], Step [400/938], Loss D: 0.6926, Loss G: 1.9536\n","Epoch [36/50], Step [500/938], Loss D: 0.7648, Loss G: 1.6450\n","Epoch [36/50], Step [600/938], Loss D: 0.8076, Loss G: 1.5514\n","Epoch [36/50], Step [700/938], Loss D: 0.8914, Loss G: 1.6553\n","Epoch [36/50], Step [800/938], Loss D: 0.7203, Loss G: 1.6715\n","Epoch [36/50], Step [900/938], Loss D: 0.8617, Loss G: 1.7208\n","Epoch [37/50], Step [100/938], Loss D: 0.8868, Loss G: 1.6783\n","Epoch [37/50], Step [200/938], Loss D: 0.6108, Loss G: 1.8292\n","Epoch [37/50], Step [300/938], Loss D: 0.7796, Loss G: 1.5241\n","Epoch [37/50], Step [400/938], Loss D: 0.8592, Loss G: 2.1148\n","Epoch [37/50], Step [500/938], Loss D: 0.8391, Loss G: 1.5144\n","Epoch [37/50], Step [600/938], Loss D: 0.8167, Loss G: 1.6817\n","Epoch [37/50], Step [700/938], Loss D: 0.9598, Loss G: 1.4160\n","Epoch [37/50], Step [800/938], Loss D: 0.8580, Loss G: 1.5229\n","Epoch [37/50], Step [900/938], Loss D: 0.8765, Loss G: 1.5021\n","Epoch [38/50], Step [100/938], Loss D: 0.9877, Loss G: 1.4761\n","Epoch [38/50], Step [200/938], Loss D: 0.8817, Loss G: 1.4218\n","Epoch [38/50], Step [300/938], Loss D: 1.0902, Loss G: 1.2879\n","Epoch [38/50], Step [400/938], Loss D: 0.9475, Loss G: 1.4872\n","Epoch [38/50], Step [500/938], Loss D: 0.7972, Loss G: 1.8264\n","Epoch [38/50], Step [600/938], Loss D: 1.0270, Loss G: 1.5409\n","Epoch [38/50], Step [700/938], Loss D: 0.9288, Loss G: 1.9211\n","Epoch [38/50], Step [800/938], Loss D: 0.9322, Loss G: 1.4552\n","Epoch [38/50], Step [900/938], Loss D: 0.7437, Loss G: 1.3788\n","Epoch [39/50], Step [100/938], Loss D: 0.8986, Loss G: 1.6963\n","Epoch [39/50], Step [200/938], Loss D: 0.7342, Loss G: 1.6610\n","Epoch [39/50], Step [300/938], Loss D: 0.9485, Loss G: 1.3384\n","Epoch [39/50], Step [400/938], Loss D: 0.9211, Loss G: 1.6954\n","Epoch [39/50], Step [500/938], Loss D: 0.7019, Loss G: 1.7858\n","Epoch [39/50], Step [600/938], Loss D: 0.9660, Loss G: 1.2855\n","Epoch [39/50], Step [700/938], Loss D: 0.8194, Loss G: 1.5971\n","Epoch [39/50], Step [800/938], Loss D: 0.7545, Loss G: 1.5626\n","Epoch [39/50], Step [900/938], Loss D: 0.9921, Loss G: 1.4251\n","Epoch [40/50], Step [100/938], Loss D: 0.7943, Loss G: 1.6953\n","Epoch [40/50], Step [200/938], Loss D: 0.8179, Loss G: 1.6685\n","Epoch [40/50], Step [300/938], Loss D: 0.7883, Loss G: 1.4536\n","Epoch [40/50], Step [400/938], Loss D: 0.8165, Loss G: 1.4258\n","Epoch [40/50], Step [500/938], Loss D: 0.9626, Loss G: 1.2997\n","Epoch [40/50], Step [600/938], Loss D: 0.8358, Loss G: 1.6533\n","Epoch [40/50], Step [700/938], Loss D: 1.0188, Loss G: 1.5059\n","Epoch [40/50], Step [800/938], Loss D: 0.9095, Loss G: 1.4684\n","Epoch [40/50], Step [900/938], Loss D: 0.9729, Loss G: 1.2514\n","Epoch [41/50], Step [100/938], Loss D: 0.8707, Loss G: 1.5817\n","Epoch [41/50], Step [200/938], Loss D: 1.0075, Loss G: 1.3806\n","Epoch [41/50], Step [300/938], Loss D: 0.9830, Loss G: 1.4310\n","Epoch [41/50], Step [400/938], Loss D: 0.7317, Loss G: 1.9164\n","Epoch [41/50], Step [500/938], Loss D: 0.6962, Loss G: 1.5295\n","Epoch [41/50], Step [600/938], Loss D: 0.8806, Loss G: 1.6885\n","Epoch [41/50], Step [700/938], Loss D: 0.8690, Loss G: 1.4396\n","Epoch [41/50], Step [800/938], Loss D: 0.7511, Loss G: 1.5287\n","Epoch [41/50], Step [900/938], Loss D: 0.9400, Loss G: 1.7703\n","Epoch [42/50], Step [100/938], Loss D: 1.0547, Loss G: 1.5329\n","Epoch [42/50], Step [200/938], Loss D: 0.8988, Loss G: 1.5893\n","Epoch [42/50], Step [300/938], Loss D: 0.9134, Loss G: 1.2902\n","Epoch [42/50], Step [400/938], Loss D: 1.0530, Loss G: 1.6851\n","Epoch [42/50], Step [500/938], Loss D: 0.8243, Loss G: 1.5925\n","Epoch [42/50], Step [600/938], Loss D: 0.9649, Loss G: 1.3191\n","Epoch [42/50], Step [700/938], Loss D: 0.9212, Loss G: 1.5568\n","Epoch [42/50], Step [800/938], Loss D: 1.0135, Loss G: 1.6685\n","Epoch [42/50], Step [900/938], Loss D: 0.7222, Loss G: 1.6430\n","Epoch [43/50], Step [100/938], Loss D: 0.8256, Loss G: 1.5731\n","Epoch [43/50], Step [200/938], Loss D: 1.0110, Loss G: 1.3254\n","Epoch [43/50], Step [300/938], Loss D: 0.9893, Loss G: 1.0641\n","Epoch [43/50], Step [400/938], Loss D: 0.9607, Loss G: 1.5431\n","Epoch [43/50], Step [500/938], Loss D: 0.7250, Loss G: 1.5143\n","Epoch [43/50], Step [600/938], Loss D: 1.0016, Loss G: 1.2334\n","Epoch [43/50], Step [700/938], Loss D: 0.8288, Loss G: 1.9380\n","Epoch [43/50], Step [800/938], Loss D: 0.7650, Loss G: 1.9034\n","Epoch [43/50], Step [900/938], Loss D: 0.9408, Loss G: 1.2876\n","Epoch [44/50], Step [100/938], Loss D: 1.0673, Loss G: 1.1451\n","Epoch [44/50], Step [200/938], Loss D: 0.8965, Loss G: 1.4915\n","Epoch [44/50], Step [300/938], Loss D: 0.9383, Loss G: 1.4616\n","Epoch [44/50], Step [400/938], Loss D: 0.8561, Loss G: 1.4277\n","Epoch [44/50], Step [500/938], Loss D: 1.2144, Loss G: 1.3400\n","Epoch [44/50], Step [600/938], Loss D: 0.9881, Loss G: 1.2337\n","Epoch [44/50], Step [700/938], Loss D: 1.1326, Loss G: 1.6779\n","Epoch [44/50], Step [800/938], Loss D: 1.0091, Loss G: 1.4621\n","Epoch [44/50], Step [900/938], Loss D: 0.8008, Loss G: 1.6019\n","Epoch [45/50], Step [100/938], Loss D: 0.8542, Loss G: 1.6379\n","Epoch [45/50], Step [200/938], Loss D: 1.0711, Loss G: 1.4389\n","Epoch [45/50], Step [300/938], Loss D: 0.9089, Loss G: 1.5108\n","Epoch [45/50], Step [400/938], Loss D: 0.9410, Loss G: 1.7178\n","Epoch [45/50], Step [500/938], Loss D: 0.9301, Loss G: 1.2953\n","Epoch [45/50], Step [600/938], Loss D: 0.9196, Loss G: 1.3413\n","Epoch [45/50], Step [700/938], Loss D: 0.9433, Loss G: 1.4597\n","Epoch [45/50], Step [800/938], Loss D: 0.9490, Loss G: 1.4320\n","Epoch [45/50], Step [900/938], Loss D: 0.8625, Loss G: 1.6865\n","Epoch [46/50], Step [100/938], Loss D: 0.8548, Loss G: 1.4010\n","Epoch [46/50], Step [200/938], Loss D: 0.8710, Loss G: 1.4268\n","Epoch [46/50], Step [300/938], Loss D: 0.7376, Loss G: 1.5569\n","Epoch [46/50], Step [400/938], Loss D: 0.9169, Loss G: 1.4718\n","Epoch [46/50], Step [500/938], Loss D: 0.9062, Loss G: 1.6532\n","Epoch [46/50], Step [600/938], Loss D: 0.7964, Loss G: 1.4919\n","Epoch [46/50], Step [700/938], Loss D: 0.8083, Loss G: 1.6173\n","Epoch [46/50], Step [800/938], Loss D: 0.9373, Loss G: 1.6365\n","Epoch [46/50], Step [900/938], Loss D: 0.9583, Loss G: 1.3788\n","Epoch [47/50], Step [100/938], Loss D: 0.8064, Loss G: 1.5695\n","Epoch [47/50], Step [200/938], Loss D: 1.0783, Loss G: 1.2682\n","Epoch [47/50], Step [300/938], Loss D: 0.9021, Loss G: 1.4965\n","Epoch [47/50], Step [400/938], Loss D: 0.9992, Loss G: 1.2237\n","Epoch [47/50], Step [500/938], Loss D: 0.7755, Loss G: 1.7649\n","Epoch [47/50], Step [600/938], Loss D: 0.6173, Loss G: 1.7072\n","Epoch [47/50], Step [700/938], Loss D: 0.9239, Loss G: 1.6537\n","Epoch [47/50], Step [800/938], Loss D: 0.8439, Loss G: 1.5836\n","Epoch [47/50], Step [900/938], Loss D: 0.8985, Loss G: 1.4673\n","Epoch [48/50], Step [100/938], Loss D: 1.0643, Loss G: 1.3026\n","Epoch [48/50], Step [200/938], Loss D: 1.0809, Loss G: 1.2561\n","Epoch [48/50], Step [300/938], Loss D: 0.8919, Loss G: 1.3748\n","Epoch [48/50], Step [400/938], Loss D: 0.8607, Loss G: 1.2771\n","Epoch [48/50], Step [500/938], Loss D: 0.9127, Loss G: 1.6833\n","Epoch [48/50], Step [600/938], Loss D: 0.7670, Loss G: 1.6186\n","Epoch [48/50], Step [700/938], Loss D: 0.9160, Loss G: 1.4690\n","Epoch [48/50], Step [800/938], Loss D: 0.8581, Loss G: 1.5986\n","Epoch [48/50], Step [900/938], Loss D: 0.9497, Loss G: 1.5547\n","Epoch [49/50], Step [100/938], Loss D: 0.8695, Loss G: 1.6410\n","Epoch [49/50], Step [200/938], Loss D: 0.9557, Loss G: 1.4744\n","Epoch [49/50], Step [300/938], Loss D: 0.9369, Loss G: 1.4937\n","Epoch [49/50], Step [400/938], Loss D: 0.8154, Loss G: 1.5856\n","Epoch [49/50], Step [500/938], Loss D: 0.7456, Loss G: 2.0313\n","Epoch [49/50], Step [600/938], Loss D: 0.7811, Loss G: 1.3598\n","Epoch [49/50], Step [700/938], Loss D: 0.9914, Loss G: 1.5290\n","Epoch [49/50], Step [800/938], Loss D: 0.9769, Loss G: 1.4489\n","Epoch [49/50], Step [900/938], Loss D: 1.0205, Loss G: 1.1326\n","Epoch [50/50], Step [100/938], Loss D: 0.8950, Loss G: 1.5148\n","Epoch [50/50], Step [200/938], Loss D: 0.7369, Loss G: 1.5969\n","Epoch [50/50], Step [300/938], Loss D: 0.7454, Loss G: 1.6997\n","Epoch [50/50], Step [400/938], Loss D: 0.9305, Loss G: 1.6063\n","Epoch [50/50], Step [500/938], Loss D: 0.7962, Loss G: 1.4322\n","Epoch [50/50], Step [600/938], Loss D: 0.7190, Loss G: 1.8227\n","Epoch [50/50], Step [700/938], Loss D: 0.8141, Loss G: 1.6090\n","Epoch [50/50], Step [800/938], Loss D: 0.8275, Loss G: 1.5874\n","Epoch [50/50], Step [900/938], Loss D: 0.8042, Loss G: 1.3263\n"]}],"source":["# Create output directory\n","os.makedirs('mnist_gan_output', exist_ok=True)\n","\n","# Training\n","num_epochs = 50\n","noise_dim = 100\n","for epoch in range(num_epochs):\n","    for i, (images, _) in enumerate(train_loader):\n","        batch_size = images.size(0)\n","        real_labels = torch.ones(batch_size, 1).to(device)\n","        fake_labels = torch.zeros(batch_size, 1).to(device)\n","\n","        # Train Discriminator\n","        discriminator.zero_grad()\n","        outputs = discriminator(images.to(device)).view(-1, 1)\n","        loss_d_real = criterion(outputs, real_labels)\n","        loss_d_real.backward()\n","\n","        z = sample_noise(batch_size, noise_dim)\n","        fake_images = generator(z)\n","        outputs = discriminator(fake_images.detach()).view(-1, 1)\n","        loss_d_fake = criterion(outputs, fake_labels)\n","        loss_d_fake.backward()\n","\n","        loss_d = loss_d_real + loss_d_fake\n","        optimizer_d.step()\n","\n","        # Train Generator\n","        generator.zero_grad()\n","        outputs = discriminator(fake_images).view(-1, 1)\n","        loss_g = criterion(outputs, real_labels)\n","        loss_g.backward()\n","        optimizer_g.step()\n","\n","        if (i+1) % 100 == 0:\n","            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss D: {loss_d.item():.4f}, Loss G: {loss_g.item():.4f}')\n","\n","    # Save real images\n","    if (epoch+1) == 1:\n","        images = images.reshape(images.size(0), 1, 28, 28)\n","        vutils.save_image(images, 'mnist_gan_output/real_images.png', normalize=True)\n","\n","    # Save sampled images\n","    fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n","    vutils.save_image(fake_images, f'mnist_gan_output/fake_images_epoch_{epoch+1}.png', normalize=True)\n"]}]}