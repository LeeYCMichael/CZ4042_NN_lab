{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNbaA1ZPiXodOA0pShJsLwU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Chapter 8, Example 1"],"metadata":{"id":"D9OyJp5e0TF7"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"hp1IXLLuxnQ6","executionInfo":{"status":"ok","timestamp":1689227296012,"user_tz":-480,"elapsed":309,"user":{"displayName":"Chen Change Loy","userId":"05499521047689607651"}}},"outputs":[],"source":["# Import PyTorch\n","import torch\n","from torch import nn\n","import numpy as np\n","\n","class RNN(nn.Module):\n","\n","    def __init__(self):\n","        super(RNN, self).__init__()\n","\n","        # Initialize the weights and biases\n","        self.u = nn.Parameter(torch.tensor([[-1.0, 0.5, 0.2], [0.5, 0.1, -2.0]], dtype=torch.float32))\n","        self.w = nn.Parameter(torch.tensor([[2.0, 1.3, -1.0], [1.5, 0.0, -0.5 ], [-0.2, 1.5, 0.4]], dtype=torch.float32))\n","        self.v = nn.Parameter(torch.tensor([[2.0, -1.0], [-1.5, 0.5], [0.2, 0.8]], dtype=torch.float32))\n","        self.b = nn.Parameter(torch.tensor([0.2, 0.2, 0.2], dtype=torch.float32))\n","        self.c = nn.Parameter(torch.tensor([0.5, 0.1], dtype=torch.float32))\n","\n","    def forward(self, x):\n","\n","        h = torch.zeros(1, 3, dtype=x[0].dtype)  # Initial hidden vector\n","\n","        for i in range(0, len(x)):\n","            h = torch.matmul(x[i], self.u) + torch.matmul(h, self.w) + self.b\n","            h = torch.tanh(h)\n","            y = torch.matmul(h, self.v) + self.c\n","            y = torch.sigmoid(y)\n","\n","            # Print out the results during iteration\n","            print(f'h({i + 1}) = {h}')\n","            print(f'y({i + 1}) = {y}')\n","            print(' ')\n","\n","        return y\n"]},{"cell_type":"markdown","source":["This code defines a simple recurrent neural network (RNN) using the PyTorch framework. Here's a breakdown of the different parts:\n","\n","1. **Importing necessary libraries and modules**: The code begins by importing the necessary libraries. It imports `torch` for general PyTorch functionality, `nn` for neural network modules, and `numpy` for numerical computations.\n","\n","2. **Defining the RNN class**: The `RNN` class is defined, which inherits from `nn.Module`. This is a standard way to define a neural network model in PyTorch. The class has two methods: `__init__` and `forward`.\n","   - `__init__`: This method initializes the model parameters. The `super(RNN, self).__init__()` call is necessary to inherit the properties and methods of the parent class `nn.Module`. The parameters `u`, `w`, `v`, `b`, and `c` are initialized as instances of `nn.Parameter`. This class is a kind of tensor that is automatically included in the list of model parameters. In this case, these parameters are initialized with specific values provided as `torch.tensor`.\n","   - `forward`: This method defines the forward pass of the RNN. It takes a list of input tensors `x` and computes the output. The method initializes a hidden state `h` with zeros. For each input in `x`, it computes the new hidden state and the output using matrix multiplications (`torch.matmul`), additions, and the `tanh` and `sigmoid` activation functions. The intermediate values of `h` and `y` are printed out for each step.\n","\n","The `forward` method corresponds to the unfolding in time of the RNN. It computes the hidden state and the output for each time step based on the current input and the previous hidden state. The weights `u`, `w`, `v`, and the biases `b` and `c` are shared across time steps.\n","\n","After the loop over the inputs, the method returns the output `y` of the last time step. Note that this implementation only works for a single sequence and does not support batch processing. For a more flexible and efficient implementation, one would usually use PyTorch's built-in RNN modules.\n"],"metadata":{"id":"fVOQnYNlztw4"}},{"cell_type":"code","source":["# Initialize the inputs, and put them in a list\n","x1 = torch.tensor([[1, 2]], dtype=torch.float32)  # size = (1, 2)\n","x2 = torch.tensor([[-1, 1]], dtype=torch.float32)  # size = (1, 2)\n","x3 = torch.tensor([[0, 3]], dtype=torch.float32)  # size = (1, 2)\n","x4 = torch.tensor([[2, -1]], dtype=torch.float32)  # size = (1, 2)\n","x = [x1, x2, x3, x4]\n","\n","rnn = RNN()  # Initialize the RNN\n","y = rnn(x)  # Pass the inputs to the RNN"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7HnHQt5tyK2x","executionInfo":{"status":"ok","timestamp":1689227299512,"user_tz":-480,"elapsed":320,"user":{"displayName":"Chen Change Loy","userId":"05499521047689607651"}},"outputId":"718786be-49da-42d2-9846-b62c9f799d87"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["h(1) = tensor([[ 0.1974,  0.7163, -0.9985]], grad_fn=<TanhBackward0>)\n","y(1) = tensor([[0.4063, 0.3686]], grad_fn=<SigmoidBackward0>)\n"," \n","h(2) = tensor([[ 0.9976, -0.8939, -0.9946]], grad_fn=<TanhBackward0>)\n","y(2) = tensor([[0.9744, 0.1052]], grad_fn=<SigmoidBackward0>)\n"," \n","h(3) = tensor([[ 0.9880,  0.2959, -1.0000]], grad_fn=<TanhBackward0>)\n","y(3) = tensor([[0.8620, 0.1765]], grad_fn=<SigmoidBackward0>)\n"," \n","h(4) = tensor([[0.3093, 0.7086, 0.7872]], grad_fn=<TanhBackward0>)\n","y(4) = tensor([[0.5531, 0.6845]], grad_fn=<SigmoidBackward0>)\n"," \n"]}]},{"cell_type":"markdown","source":["This code snippet performs the following tasks:\n","\n","1. **Initialize the inputs**: It creates four tensors `x1`, `x2`, `x3`, and `x4` each of size (1, 2) using `torch.tensor`. These tensors are then put into a list `x`. Each tensor represents an input at a specific time step.\n","\n","2. **Initialize the RNN**: An instance of the `RNN` class is created and assigned to the variable `rnn`. This initializes the model and its parameters, as defined in the `__init__` method of the `RNN` class.\n","\n","3. **Pass the inputs to the RNN**: The list of input tensors `x` is passed to the RNN for processing. This is done by calling `rnn(x)`, which triggers the `forward` method of the `RNN` class. The forward pass computes the hidden states and the outputs for each time step based on the current input and the previous hidden state, as defined in the `forward` method of the `RNN` class. The output of the last time step is assigned to the variable `y`.\n","\n","The code essentially prepares input data and processes it through a custom-defined recurrent neural network.\n"],"metadata":{"id":"LYiY2Ip5022R"}}]}