{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO2zMakDJ3Qx3GgaOhQhbqm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Chapter 9, Example 1"],"metadata":{"id":"ji3ENTZCxWOF"}},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","\n","# Assuming we have two words x_1 and x2 with the respective embedding\n","# Define the embeddings\n","x_1 = torch.tensor([1, 2, 3], dtype=torch.float32)\n","x_2 = torch.tensor([4, 5, 6], dtype=torch.float32)\n","embeddings = torch.stack([x_1, x_2])\n","\n","# Define the projection matrices\n","W_Q = torch.tensor([[0.01, 0.03],\n","                    [0.02, 0.02],\n","                    [0.03, 0.01]], dtype=torch.float32)\n","\n","W_K = torch.tensor([[0.05, 0.05],\n","                    [0.06, 0.05],\n","                    [0.07, 0.05]], dtype=torch.float32)\n","\n","W_V = torch.tensor([[0.02, 0.02],\n","                    [0.01, 0.02],\n","                    [0.01, 0.01]], dtype=torch.float32)\n","\n","# Compute the query, key, and value representations\n","Q = torch.matmul(embeddings, W_Q)\n","K = torch.matmul(embeddings, W_K)\n","V = torch.matmul(embeddings, W_V)"],"metadata":{"id":"Js-cxl1YxpsL","executionInfo":{"status":"ok","timestamp":1697012783932,"user_tz":-480,"elapsed":340,"user":{"displayName":"Chen Change Loy","userId":"05499521047689607651"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["# Scaled Dot Product Attention\n","\n","The Scaled Dot Product Attention mechanism is a core component of the Transformer architecture, which has been influential in various NLP tasks. This mechanism computes attention scores based on the dot product of the query and key matrices, and then scales the scores to stabilize gradients, especially in deeper models or larger embeddings.\n","\n","## Steps:\n","\n","1. **Compute Dimension (`d_k`)**:\n","    - `d_k` represents the dimension of the key vectors.\n","    - It's obtained from the last dimension of the `W_K` tensor.\n","\n","2. **Calculate Attention Scores (`attn_scores`)**:\n","    - The attention scores are computed by taking the dot product of the query matrix `Q` and the transposed key matrix `K`.\n","    - In the context of the attention mechanism, the matrices `Q`, `K`, and `V` typically have the shape `(batch_size, sequence_length, d_k)`, where `d_k` represents the dimension of the key (and also the query) vectors.\n","    - The operation `matmul(Q, K)` would attempt to multiply the last dimension of `Q` with the second-to-last dimension of `K`. However, this isn't the desired behavior for the dot product. By transposing the last two dimensions of `K` using `K.transpose(-2, -1)`, we modify its shape to `(batch_size, d_k, sequence_length)`. Now, executing `matmul(Q, K.transpose(-2, -1))` multiplies the last dimension of `Q` (which is `d_k`) with the second-to-last dimension of the transposed `K` (also `d_k`), yielding the desired shape `(batch_size, sequence_length, sequence_length)`.\n","    - *Why `-2` and `-1`?* In PyTorch, negative indices for dimensions count from the last dimension backward. Specifically, `-1` refers to the last dimension, and `-2` pertains to the second-to-last dimension. Utilizing negative indices in this context ensures that the code remains general and adaptable to tensors with varying numbers of dimensions.\n","    - The scores are then scaled down by dividing by the square root of `d_k`. This scaling helps in stabilizing the gradients, especially when the dimensions of the key vectors are large.\n","\n","3. **Compute Attention Weights (`attention_weights`)**:\n","    - The scaled attention scores are passed through a softmax function along the last dimension to produce the attention weights. This ensures that the weights are normalized and sum up to 1 for each query.\n","\n","4. **Compute Attention Output (`attention_out`)**:\n","    - The attention output is computed by taking the dot product of the attention weights and the value matrix `V`. This step essentially takes a weighted sum of the values based on the attention weights, giving more importance to values that are more relevant to the given query.\n","\n","The resulting `attention_out` tensor provides a context-aware representation of the input, emphasizing the most relevant parts of the input for each query.\n","\n"],"metadata":{"id":"5ZEYTx21xvQq"}},{"cell_type":"code","source":["# Compute the Scaled Dot Product Attention\n","d_k = W_K.size(-1)  # dimension of keys\n","attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_k ** 0.5)  # scaling by sqrt(d_k)\n","attention_weights = F.softmax(attn_scores, dim=-1)\n","attention_out = torch.matmul(attention_weights, V)"],"metadata":{"id":"u1Stn6tpxtRA","executionInfo":{"status":"ok","timestamp":1697012798821,"user_tz":-480,"elapsed":302,"user":{"displayName":"Chen Change Loy","userId":"05499521047689607651"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nkCbV-vWm1IG","executionInfo":{"status":"ok","timestamp":1697012935642,"user_tz":-480,"elapsed":392,"user":{"displayName":"Chen Change Loy","userId":"05499521047689607651"}},"outputId":"b24b1089-861b-4528-b131-00b483535bb9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Query:\n"," tensor([[0.1400, 0.1000],\n","        [0.3200, 0.2800]])\n","Key:\n"," tensor([[0.3800, 0.3000],\n","        [0.9200, 0.7500]])\n","Key transposed:\n"," <built-in method transpose of Tensor object at 0x79fa72edf060>\n","Value:\n"," tensor([[0.0700, 0.0900],\n","        [0.1900, 0.2400]])\n","Attention scores:\n"," tensor([[0.0588, 0.1441],\n","        [0.1454, 0.3567]])\n","Attention weights:\n"," tensor([[0.4787, 0.5213],\n","        [0.4474, 0.5526]])\n","Self Attention Output:\n"," tensor([[0.1326, 0.1682],\n","        [0.1363, 0.1729]])\n"]}],"source":["print(\"Query:\\n\", Q)\n","print(\"Key:\\n\", K)\n","print(\"Key transposed:\\n\", K.transpose)\n","print(\"Value:\\n\", V)\n","print(\"Attention scores:\\n\", attn_scores)\n","print(\"Attention weights:\\n\", attention_weights)\n","print(\"Self Attention Output:\\n\", attention_out)"]}]}