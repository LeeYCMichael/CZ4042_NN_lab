{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOhK/JPvu8oM9OzyZY/PXAe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Chapter 8, Example 2"],"metadata":{"id":"z-xounESpipD"}},{"cell_type":"code","source":["# Import PyTorch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Helper libraries\n","import numpy as np"],"metadata":{"id":"ITGe4GsVpTaA","executionInfo":{"status":"ok","timestamp":1696908825626,"user_tz":-480,"elapsed":313,"user":{"displayName":"Chen Change Loy","userId":"05499521047689607651"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Recurrent Neural Network (RNN) Implementation"],"metadata":{"id":"DBGOmPRDqLpl"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"kRRxyYOjnxb6","executionInfo":{"status":"ok","timestamp":1696908827614,"user_tz":-480,"elapsed":545,"user":{"displayName":"Chen Change Loy","userId":"05499521047689607651"}}},"outputs":[],"source":["class RNN(nn.Module):\n","    def __init__(self):\n","        super(RNN, self).__init__()\n","\n","        # Initialize the weights and biases\n","        self.u = nn.Parameter(torch.tensor([[-1.0, 0.5, 0.2], [0.5, 0.1, -2.0]], dtype=torch.float32))\n","        self.w = nn.Parameter(torch.tensor([[2.0, 1.3, -1.0]], dtype=torch.float32))\n","        self.v = nn.Parameter(torch.tensor([[2.0], [-1.5], [0.2]], dtype=torch.float32))\n","        self.b = nn.Parameter(torch.tensor([0.2, 0.2, 0.2], dtype=torch.float32))\n","        self.c = nn.Parameter(torch.tensor([0.1], dtype=torch.float32))\n","\n","    def forward(self, x):\n","        y = torch.tensor([[0]], dtype=x[0].dtype)\n","\n","        for i in range(0, len(x)):\n","            h = torch.tanh(torch.mm(x[i], self.u) + torch.mm(y, self.w) + self.b)\n","            y = torch.sigmoid(torch.mm(h, self.v) + self.c)\n","\n","            # Print out the results during iteration\n","            print(f'h({i + 1}):')\n","            print(h)\n","            print(f'y({i + 1}):')\n","            print(y)\n","            print(' ')\n","\n","        return y"]},{"cell_type":"markdown","source":["\n","\n","The `RNN` class defines a simple recurrent neural network architecture.\n","\n","## Class Initialization:\n","\n","1. **Super Initialization**:\n","    - `super(RNN, self).__init__()`: This initializes the parent `nn.Module` class, which is essential for all PyTorch custom modules.\n","\n","2. **Weights and Biases Initialization**:\n","    - `self.u`: Weight matrix for the input `x`.\n","    - `self.w`: Weight matrix for the previous output `y`.\n","    - `self.v`: Weight matrix for the hidden state `h`.\n","    - `self.b`: Bias vector for the hidden state.\n","    - `self.c`: Bias for the output `y`.\n","\n","## Forward Method:\n","\n","This method defines the forward pass of the RNN.\n","\n","1. **Initial Output (`y`)**:\n","    - Initialized to a tensor of shape `[1, 1]` with a value of 0. This represents the initial output of the RNN.\n","\n","2. **Iteration over Input Sequence (`x`)**:\n","    - For each input in the sequence:\n","        - Compute the hidden state `h` using the formula:\n","            \\[ h = \\tanh(x \\times u + y \\times w + b) \\]\n","        - Compute the output `y` using the formula:\n","            \\[ y = \\sigma(h \\times v + c) \\]\n","        - Print the values of `h` and `y` for each iteration.\n","\n","3. **Return**:\n","    - The final output `y` is returned.\n","\n","This RNN implementation showcases the basic recurrent computations where the output from the previous step is used as an input for the current step. The use of the `tanh` activation function for the hidden state and the `sigmoid` activation function for the output is a common choice in many RNN architectures.\n"],"metadata":{"id":"QoSl_JnNqGU8"}},{"cell_type":"code","source":["x1 = torch.tensor([[1, 2], [-1, 0]], dtype=torch.float32)\n","x2 = torch.tensor([[-1, 1], [2, -1]], dtype=torch.float32)\n","x3 = torch.tensor([[0, 3], [3, -1]], dtype=torch.float32)\n","x = [x1, x2, x3]"],"metadata":{"id":"MfZI-l-upYHV","executionInfo":{"status":"ok","timestamp":1696908830906,"user_tz":-480,"elapsed":305,"user":{"displayName":"Chen Change Loy","userId":"05499521047689607651"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["rnn = RNN()  # Initialize\n","y = rnn(x)  # Compute the outputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZtQoMTGRpZCr","executionInfo":{"status":"ok","timestamp":1696908832261,"user_tz":-480,"elapsed":6,"user":{"displayName":"Chen Change Loy","userId":"05499521047689607651"}},"outputId":"0ef88a89-0aa7-4e97-c7c1-c3e6323e7254"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["h(1):\n","tensor([[ 0.1974,  0.7163, -0.9985],\n","        [ 0.8337, -0.2913,  0.0000]], grad_fn=<TanhBackward0>)\n","y(1):\n","tensor([[0.3144],\n","        [0.9006]], grad_fn=<SigmoidBackward0>)\n"," \n","h(2):\n","tensor([[ 0.9812,  0.2058, -0.9807],\n","        [-0.4611,  0.9789,  0.9353]], grad_fn=<TanhBackward0>)\n","y(2):\n","tensor([[0.8260],\n","        [0.1088]], grad_fn=<SigmoidBackward0>)\n"," \n","h(3):\n","tensor([[ 0.9976,  0.9176, -1.0000],\n","        [-0.9958,  0.9404,  0.9908]], grad_fn=<TanhBackward0>)\n","y(3):\n","tensor([[0.6268],\n","        [0.0429]], grad_fn=<SigmoidBackward0>)\n"," \n"]}]}]}