{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFVxWZGJxprU"
   },
   "source": [
    "# CS4001/4042 Assignment 1, Part B, Q2\n",
    "In Question B1, we used the Category Embedding model. This creates a feedforward neural network in which the categorical features get learnable embeddings. In this question, we will make use of a library called Pytorch-WideDeep. This library makes it easy to work with multimodal deep-learning problems combining images, text, and tables. We will just be utilizing the deeptabular component of this library through the TabMlp network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EycCozG06Duu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-widedeep\n",
      "  Obtaining dependency information for pytorch-widedeep from https://files.pythonhosted.org/packages/17/f4/48f8d4c527baea10808b822fd3c00260f2b3b453937f2ef54bc464da1b88/pytorch_widedeep-1.3.2-py3-none-any.whl.metadata\n",
      "  Using cached pytorch_widedeep-1.3.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas>=1.3.5 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from pytorch-widedeep) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from pytorch-widedeep) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.7.3 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from pytorch-widedeep) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from pytorch-widedeep) (1.3.0)\n",
      "Collecting gensim (from pytorch-widedeep)\n",
      "  Obtaining dependency information for gensim from https://files.pythonhosted.org/packages/3e/b7/fba98a65efea29a7d8bf25ade2db67e34ebab8e63769e8927d0a4d42a84f/gensim-4.3.2-cp38-cp38-win_amd64.whl.metadata\n",
      "  Using cached gensim-4.3.2-cp38-cp38-win_amd64.whl.metadata (8.5 kB)\n",
      "Collecting spacy (from pytorch-widedeep)\n",
      "  Obtaining dependency information for spacy from https://files.pythonhosted.org/packages/14/26/7447496e90ee51bf00d1af33085c180eeb26166149bed1d30ef4c53d862c/spacy-3.6.1-cp38-cp38-win_amd64.whl.metadata\n",
      "  Using cached spacy-3.6.1-cp38-cp38-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from pytorch-widedeep) (4.8.0.76)\n",
      "Requirement already satisfied: imutils in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from pytorch-widedeep) (0.5.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from pytorch-widedeep) (4.66.1)\n",
      "Requirement already satisfied: torch in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from pytorch-widedeep) (2.0.1)\n",
      "Collecting torchvision (from pytorch-widedeep)\n",
      "  Using cached torchvision-0.15.2-cp38-cp38-win_amd64.whl (1.2 MB)\n",
      "Requirement already satisfied: einops in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from pytorch-widedeep) (0.6.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from pytorch-widedeep) (1.15.0)\n",
      "Requirement already satisfied: torchmetrics in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from pytorch-widedeep) (0.11.4)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from pytorch-widedeep) (13.0.0)\n",
      "Collecting fastparquet>=0.8.1 (from pytorch-widedeep)\n",
      "  Obtaining dependency information for fastparquet>=0.8.1 from https://files.pythonhosted.org/packages/22/86/8da774a8d52493f9bd3651571af2edd3d369d7e0affb79b7fa5d4aec334b/fastparquet-2023.8.0-cp38-cp38-win_amd64.whl.metadata\n",
      "  Using cached fastparquet-2023.8.0-cp38-cp38-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: cramjam>=2.3 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from fastparquet>=0.8.1->pytorch-widedeep) (2.7.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from fastparquet>=0.8.1->pytorch-widedeep) (2023.9.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from fastparquet>=0.8.1->pytorch-widedeep) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from pandas>=1.3.5->pytorch-widedeep) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from pandas>=1.3.5->pytorch-widedeep) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from pandas>=1.3.5->pytorch-widedeep) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (3.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from gensim->pytorch-widedeep) (6.4.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from spacy->pytorch-widedeep) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from spacy->pytorch-widedeep) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from spacy->pytorch-widedeep) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from spacy->pytorch-widedeep) (2.0.8)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy->pytorch-widedeep)\n",
      "  Obtaining dependency information for preshed<3.1.0,>=3.0.2 from https://files.pythonhosted.org/packages/11/d6/52e305ae0d66769c926a02a1f0e87f9918a25a68360de83365b1ff902ef0/preshed-3.0.9-cp38-cp38-win_amd64.whl.metadata\n",
      "  Using cached preshed-3.0.9-cp38-cp38-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.8 (from spacy->pytorch-widedeep)\n",
      "  Obtaining dependency information for thinc<8.2.0,>=8.1.8 from https://files.pythonhosted.org/packages/dc/ad/253d28e0203e50c4361693a43ef40b6022e625ea28792f1e7215ff8e7034/thinc-8.1.12-cp38-cp38-win_amd64.whl.metadata\n",
      "  Using cached thinc-8.1.12-cp38-cp38-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from spacy->pytorch-widedeep) (1.1.2)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy->pytorch-widedeep)\n",
      "  Obtaining dependency information for srsly<3.0.0,>=2.4.3 from https://files.pythonhosted.org/packages/0d/47/65006f95cfac4d20a941cb2c92c6d999e084976d207a2720e9f486ffd7d7/srsly-2.4.7-cp38-cp38-win_amd64.whl.metadata\n",
      "  Using cached srsly-2.4.7-cp38-cp38-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from spacy->pytorch-widedeep) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from spacy->pytorch-widedeep) (0.9.0)\n",
      "Collecting pathy>=0.10.0 (from spacy->pytorch-widedeep)\n",
      "  Obtaining dependency information for pathy>=0.10.0 from https://files.pythonhosted.org/packages/b5/c3/04a002ace658133f5ac48d30258ed9ceab720595dc1ac36df02fe52018af/pathy-0.10.2-py3-none-any.whl.metadata\n",
      "  Using cached pathy-0.10.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from spacy->pytorch-widedeep) (2.31.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy->pytorch-widedeep)\n",
      "  Obtaining dependency information for pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 from https://files.pythonhosted.org/packages/82/06/fafdc75e48b248eff364b4249af4bcc6952225e8f20e8205820afc66e88e/pydantic-2.3.0-py3-none-any.whl.metadata\n",
      "  Using cached pydantic-2.3.0-py3-none-any.whl.metadata (148 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from spacy->pytorch-widedeep) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from spacy->pytorch-widedeep) (68.0.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from spacy->pytorch-widedeep) (3.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from tqdm->pytorch-widedeep) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from torch->pytorch-widedeep) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from torch->pytorch-widedeep) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from torch->pytorch-widedeep) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from torch->pytorch-widedeep) (3.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from torchvision->pytorch-widedeep) (10.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->pytorch-widedeep) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->pytorch-widedeep) (2.6.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.5->pytorch-widedeep) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy->pytorch-widedeep) (0.7.10)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.2.0,>=8.1.8->spacy->pytorch-widedeep)\n",
      "  Obtaining dependency information for confection<1.0.0,>=0.0.1 from https://files.pythonhosted.org/packages/93/f8/e89268a1f885048fb2ee6b5c9f93c4e90de768534acfef3652f87d97d4cb/confection-0.1.3-py3-none-any.whl.metadata\n",
      "  Using cached confection-0.1.3-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy->pytorch-widedeep) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from jinja2->spacy->pytorch-widedeep) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages (from sympy->torch->pytorch-widedeep) (1.3.0)\n",
      "Using cached pytorch_widedeep-1.3.2-py3-none-any.whl (21.8 MB)\n",
      "Using cached fastparquet-2023.8.0-cp38-cp38-win_amd64.whl (715 kB)\n",
      "Using cached gensim-4.3.2-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "Using cached spacy-3.6.1-cp38-cp38-win_amd64.whl (12.4 MB)\n",
      "Using cached pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "Using cached preshed-3.0.9-cp38-cp38-win_amd64.whl (122 kB)\n",
      "Using cached pydantic-2.3.0-py3-none-any.whl (374 kB)\n",
      "Using cached srsly-2.4.7-cp38-cp38-win_amd64.whl (483 kB)\n",
      "Using cached thinc-8.1.12-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Using cached confection-0.1.3-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: srsly, preshed, pydantic, gensim, torchvision, pathy, fastparquet, confection, thinc, spacy, pytorch-widedeep\n",
      "Successfully installed confection-0.1.3 fastparquet-2023.8.0 gensim-4.3.2 pathy-0.10.2 preshed-3.0.9 pydantic-2.3.0 pytorch-widedeep-1.3.2 spacy-3.6.1 srsly-2.4.7 thinc-8.1.12 torchvision-0.15.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-widedeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lq0elU0J53Yo"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "import os\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pytorch_widedeep.preprocessing import TabPreprocessor\n",
    "from pytorch_widedeep.models import TabMlp, WideDeep\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.metrics import R2Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU3xdVpwzuLx"
   },
   "source": [
    ">Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from the year 2020 and before as training data, and entries from 2021 and after as the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_oYG6lNIh7Mp"
   },
   "outputs": [],
   "source": [
    "# TODO: Enter your code here\n",
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "train_df = df[df['year'] <= 2020]\n",
    "test_df = df[df['year'] >= 2021]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_q9PoR50JAA"
   },
   "source": [
    ">Refer to the documentation of Pytorch-WideDeep and perform the following tasks:\n",
    "https://pytorch-widedeep.readthedocs.io/en/latest/index.html\n",
    "* Use [**TabPreprocessor**](https://pytorch-widedeep.readthedocs.io/en/latest/examples/01_preprocessors_and_utils.html#2-tabpreprocessor) to create the deeptabular component using the continuous\n",
    "features and the categorical features. Use this component to transform the training dataset.\n",
    "* Create the [**TabMlp**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/model_components.html#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp) model with 2 linear layers in the MLP, with 200 and 100 neurons respectively.\n",
    "* Create a [**Trainer**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/trainer.html#pytorch_widedeep.training.Trainer) for the training of the created TabMlp model with the root mean squared error (RMSE) cost function. Train the model for 100 epochs using this trainer, keeping a batch size of 64. (Note: set the *num_workers* parameter to 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZBY1iqUXtYWn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\miniconda3\\envs\\cz4042_nn_alt\\lib\\site-packages\\pytorch_widedeep\\preprocessing\\tab_preprocessor.py:334: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n",
      "epoch 1: 100%|██████████| 1366/1366 [00:14<00:00, 92.16it/s, loss=2.01e+5, metrics={'r2': -1.6453}] \n",
      "epoch 2: 100%|██████████| 1366/1366 [00:11<00:00, 114.78it/s, loss=8.21e+4, metrics={'r2': 0.6774}]\n",
      "epoch 3: 100%|██████████| 1366/1366 [00:11<00:00, 122.15it/s, loss=7.29e+4, metrics={'r2': 0.759}] \n",
      "epoch 4: 100%|██████████| 1366/1366 [00:10<00:00, 133.08it/s, loss=6.96e+4, metrics={'r2': 0.7838}]\n",
      "epoch 5: 100%|██████████| 1366/1366 [00:11<00:00, 122.63it/s, loss=6.72e+4, metrics={'r2': 0.799}] \n",
      "epoch 6: 100%|██████████| 1366/1366 [00:16<00:00, 85.10it/s, loss=6.57e+4, metrics={'r2': 0.8079}] \n",
      "epoch 7: 100%|██████████| 1366/1366 [00:14<00:00, 93.40it/s, loss=6.48e+4, metrics={'r2': 0.8133}] \n",
      "epoch 8: 100%|██████████| 1366/1366 [00:11<00:00, 122.35it/s, loss=6.4e+4, metrics={'r2': 0.8173}] \n",
      "epoch 9: 100%|██████████| 1366/1366 [00:13<00:00, 101.06it/s, loss=6.35e+4, metrics={'r2': 0.8203}]\n",
      "epoch 10: 100%|██████████| 1366/1366 [00:12<00:00, 112.80it/s, loss=6.3e+4, metrics={'r2': 0.8228}] \n",
      "epoch 11: 100%|██████████| 1366/1366 [00:14<00:00, 96.29it/s, loss=6.26e+4, metrics={'r2': 0.8253}] \n",
      "epoch 12: 100%|██████████| 1366/1366 [00:16<00:00, 81.25it/s, loss=6.22e+4, metrics={'r2': 0.8269}] \n",
      "epoch 13: 100%|██████████| 1366/1366 [00:15<00:00, 90.17it/s, loss=6.21e+4, metrics={'r2': 0.8273}] \n",
      "epoch 14: 100%|██████████| 1366/1366 [00:14<00:00, 95.97it/s, loss=6.19e+4, metrics={'r2': 0.8284}] \n",
      "epoch 15: 100%|██████████| 1366/1366 [00:18<00:00, 73.03it/s, loss=6.16e+4, metrics={'r2': 0.8301}]\n",
      "epoch 16: 100%|██████████| 1366/1366 [00:16<00:00, 80.91it/s, loss=6.15e+4, metrics={'r2': 0.8308}]\n",
      "epoch 17: 100%|██████████| 1366/1366 [00:14<00:00, 91.39it/s, loss=6.12e+4, metrics={'r2': 0.8326}] \n",
      "epoch 18: 100%|██████████| 1366/1366 [00:13<00:00, 104.74it/s, loss=6.08e+4, metrics={'r2': 0.8343}]\n",
      "epoch 19: 100%|██████████| 1366/1366 [00:12<00:00, 111.91it/s, loss=6.11e+4, metrics={'r2': 0.8329}]\n",
      "epoch 20: 100%|██████████| 1366/1366 [00:10<00:00, 126.06it/s, loss=6.07e+4, metrics={'r2': 0.8348}]\n",
      "epoch 21: 100%|██████████| 1366/1366 [00:11<00:00, 118.19it/s, loss=6.06e+4, metrics={'r2': 0.8356}]\n",
      "epoch 22: 100%|██████████| 1366/1366 [00:11<00:00, 121.93it/s, loss=6.05e+4, metrics={'r2': 0.8357}]\n",
      "epoch 23: 100%|██████████| 1366/1366 [00:10<00:00, 130.29it/s, loss=6.04e+4, metrics={'r2': 0.8363}]\n",
      "epoch 24: 100%|██████████| 1366/1366 [00:10<00:00, 130.75it/s, loss=6.03e+4, metrics={'r2': 0.8368}]\n",
      "epoch 25: 100%|██████████| 1366/1366 [00:10<00:00, 132.31it/s, loss=6.02e+4, metrics={'r2': 0.8371}]\n",
      "epoch 26: 100%|██████████| 1366/1366 [00:10<00:00, 133.73it/s, loss=6.02e+4, metrics={'r2': 0.8373}]\n",
      "epoch 27: 100%|██████████| 1366/1366 [00:10<00:00, 133.11it/s, loss=6e+4, metrics={'r2': 0.8386}]   \n",
      "epoch 28: 100%|██████████| 1366/1366 [00:10<00:00, 132.82it/s, loss=5.97e+4, metrics={'r2': 0.8398}]\n",
      "epoch 29: 100%|██████████| 1366/1366 [00:10<00:00, 131.35it/s, loss=5.97e+4, metrics={'r2': 0.8398}]\n",
      "epoch 30: 100%|██████████| 1366/1366 [00:10<00:00, 128.58it/s, loss=5.97e+4, metrics={'r2': 0.8395}]\n",
      "epoch 31: 100%|██████████| 1366/1366 [00:11<00:00, 122.00it/s, loss=5.97e+4, metrics={'r2': 0.8398}]\n",
      "epoch 32: 100%|██████████| 1366/1366 [00:10<00:00, 126.66it/s, loss=5.97e+4, metrics={'r2': 0.8397}]\n",
      "epoch 33: 100%|██████████| 1366/1366 [00:10<00:00, 130.54it/s, loss=5.96e+4, metrics={'r2': 0.8406}]\n",
      "epoch 34: 100%|██████████| 1366/1366 [00:10<00:00, 131.54it/s, loss=5.96e+4, metrics={'r2': 0.8405}]\n",
      "epoch 35: 100%|██████████| 1366/1366 [00:11<00:00, 121.15it/s, loss=5.95e+4, metrics={'r2': 0.8404}]\n",
      "epoch 36: 100%|██████████| 1366/1366 [00:10<00:00, 128.18it/s, loss=5.97e+4, metrics={'r2': 0.8397}]\n",
      "epoch 37: 100%|██████████| 1366/1366 [00:10<00:00, 124.70it/s, loss=5.93e+4, metrics={'r2': 0.8413}]\n",
      "epoch 38: 100%|██████████| 1366/1366 [00:13<00:00, 103.50it/s, loss=5.93e+4, metrics={'r2': 0.8419}]\n",
      "epoch 39: 100%|██████████| 1366/1366 [00:12<00:00, 111.86it/s, loss=5.94e+4, metrics={'r2': 0.8408}]\n",
      "epoch 40: 100%|██████████| 1366/1366 [00:10<00:00, 131.98it/s, loss=5.9e+4, metrics={'r2': 0.8433}] \n",
      "epoch 41: 100%|██████████| 1366/1366 [00:10<00:00, 126.13it/s, loss=5.91e+4, metrics={'r2': 0.8424}]\n",
      "epoch 42: 100%|██████████| 1366/1366 [00:10<00:00, 124.87it/s, loss=5.9e+4, metrics={'r2': 0.8435}] \n",
      "epoch 43: 100%|██████████| 1366/1366 [00:14<00:00, 97.36it/s, loss=5.9e+4, metrics={'r2': 0.843}]   \n",
      "epoch 44: 100%|██████████| 1366/1366 [00:11<00:00, 119.22it/s, loss=5.91e+4, metrics={'r2': 0.8426}]\n",
      "epoch 45: 100%|██████████| 1366/1366 [00:13<00:00, 100.60it/s, loss=5.9e+4, metrics={'r2': 0.8434}] \n",
      "epoch 46: 100%|██████████| 1366/1366 [00:15<00:00, 87.38it/s, loss=5.92e+4, metrics={'r2': 0.8422}] \n",
      "epoch 47: 100%|██████████| 1366/1366 [00:10<00:00, 125.00it/s, loss=5.88e+4, metrics={'r2': 0.8443}]\n",
      "epoch 48: 100%|██████████| 1366/1366 [00:12<00:00, 113.14it/s, loss=5.9e+4, metrics={'r2': 0.8429}] \n",
      "epoch 49: 100%|██████████| 1366/1366 [00:17<00:00, 78.15it/s, loss=5.89e+4, metrics={'r2': 0.8436}] \n",
      "epoch 50: 100%|██████████| 1366/1366 [00:12<00:00, 111.79it/s, loss=5.88e+4, metrics={'r2': 0.844}] \n",
      "epoch 51: 100%|██████████| 1366/1366 [00:19<00:00, 69.56it/s, loss=5.87e+4, metrics={'r2': 0.8442}]\n",
      "epoch 52: 100%|██████████| 1366/1366 [00:18<00:00, 75.15it/s, loss=5.89e+4, metrics={'r2': 0.8434}] \n",
      "epoch 53: 100%|██████████| 1366/1366 [00:12<00:00, 108.22it/s, loss=5.87e+4, metrics={'r2': 0.8442}]\n",
      "epoch 54: 100%|██████████| 1366/1366 [00:12<00:00, 107.09it/s, loss=5.89e+4, metrics={'r2': 0.8437}]\n",
      "epoch 55: 100%|██████████| 1366/1366 [00:14<00:00, 94.04it/s, loss=5.86e+4, metrics={'r2': 0.8454}] \n",
      "epoch 56: 100%|██████████| 1366/1366 [00:13<00:00, 100.01it/s, loss=5.87e+4, metrics={'r2': 0.8442}]\n",
      "epoch 57: 100%|██████████| 1366/1366 [00:16<00:00, 80.70it/s, loss=5.86e+4, metrics={'r2': 0.8452}] \n",
      "epoch 58: 100%|██████████| 1366/1366 [00:13<00:00, 99.02it/s, loss=5.86e+4, metrics={'r2': 0.8449}] \n",
      "epoch 59: 100%|██████████| 1366/1366 [00:12<00:00, 111.62it/s, loss=5.85e+4, metrics={'r2': 0.8453}]\n",
      "epoch 60: 100%|██████████| 1366/1366 [00:13<00:00, 98.31it/s, loss=5.85e+4, metrics={'r2': 0.8456}] \n",
      "epoch 61: 100%|██████████| 1366/1366 [00:12<00:00, 108.84it/s, loss=5.84e+4, metrics={'r2': 0.8462}]\n",
      "epoch 62: 100%|██████████| 1366/1366 [00:11<00:00, 123.05it/s, loss=5.86e+4, metrics={'r2': 0.8451}]\n",
      "epoch 63: 100%|██████████| 1366/1366 [00:10<00:00, 125.45it/s, loss=5.84e+4, metrics={'r2': 0.8462}]\n",
      "epoch 64: 100%|██████████| 1366/1366 [00:12<00:00, 110.98it/s, loss=5.83e+4, metrics={'r2': 0.8465}]\n",
      "epoch 65: 100%|██████████| 1366/1366 [00:12<00:00, 111.59it/s, loss=5.84e+4, metrics={'r2': 0.8458}]\n",
      "epoch 66: 100%|██████████| 1366/1366 [00:11<00:00, 121.33it/s, loss=5.83e+4, metrics={'r2': 0.8463}]\n",
      "epoch 67: 100%|██████████| 1366/1366 [00:11<00:00, 116.56it/s, loss=5.83e+4, metrics={'r2': 0.8465}]\n",
      "epoch 68: 100%|██████████| 1366/1366 [00:11<00:00, 123.16it/s, loss=5.83e+4, metrics={'r2': 0.8466}]\n",
      "epoch 69: 100%|██████████| 1366/1366 [00:11<00:00, 115.45it/s, loss=5.84e+4, metrics={'r2': 0.8461}]\n",
      "epoch 70: 100%|██████████| 1366/1366 [00:11<00:00, 116.72it/s, loss=5.83e+4, metrics={'r2': 0.8461}]\n",
      "epoch 71: 100%|██████████| 1366/1366 [00:11<00:00, 123.04it/s, loss=5.82e+4, metrics={'r2': 0.847}] \n",
      "epoch 72: 100%|██████████| 1366/1366 [00:10<00:00, 128.08it/s, loss=5.82e+4, metrics={'r2': 0.8473}]\n",
      "epoch 73: 100%|██████████| 1366/1366 [00:10<00:00, 125.30it/s, loss=5.81e+4, metrics={'r2': 0.8475}]\n",
      "epoch 74: 100%|██████████| 1366/1366 [00:11<00:00, 114.00it/s, loss=5.82e+4, metrics={'r2': 0.8469}]\n",
      "epoch 75: 100%|██████████| 1366/1366 [00:14<00:00, 93.17it/s, loss=5.82e+4, metrics={'r2': 0.8469}] \n",
      "epoch 76: 100%|██████████| 1366/1366 [00:12<00:00, 105.66it/s, loss=5.82e+4, metrics={'r2': 0.8469}]\n",
      "epoch 77: 100%|██████████| 1366/1366 [00:18<00:00, 73.75it/s, loss=5.8e+4, metrics={'r2': 0.8482}]  \n",
      "epoch 78: 100%|██████████| 1366/1366 [00:16<00:00, 82.10it/s, loss=5.81e+4, metrics={'r2': 0.8475}] \n",
      "epoch 79: 100%|██████████| 1366/1366 [00:12<00:00, 112.15it/s, loss=5.79e+4, metrics={'r2': 0.8485}]\n",
      "epoch 80: 100%|██████████| 1366/1366 [00:10<00:00, 125.05it/s, loss=5.78e+4, metrics={'r2': 0.8491}]\n",
      "epoch 81: 100%|██████████| 1366/1366 [00:11<00:00, 123.52it/s, loss=5.79e+4, metrics={'r2': 0.8483}]\n",
      "epoch 82: 100%|██████████| 1366/1366 [00:10<00:00, 125.63it/s, loss=5.78e+4, metrics={'r2': 0.8492}]\n",
      "epoch 83: 100%|██████████| 1366/1366 [00:13<00:00, 97.95it/s, loss=5.77e+4, metrics={'r2': 0.8494}]\n",
      "epoch 84: 100%|██████████| 1366/1366 [00:15<00:00, 89.84it/s, loss=5.78e+4, metrics={'r2': 0.8485}] \n",
      "epoch 85: 100%|██████████| 1366/1366 [00:12<00:00, 110.49it/s, loss=5.78e+4, metrics={'r2': 0.8489}]\n",
      "epoch 86: 100%|██████████| 1366/1366 [00:12<00:00, 111.10it/s, loss=5.78e+4, metrics={'r2': 0.8491}]\n",
      "epoch 87: 100%|██████████| 1366/1366 [00:13<00:00, 102.34it/s, loss=5.77e+4, metrics={'r2': 0.8494}]\n",
      "epoch 88: 100%|██████████| 1366/1366 [00:13<00:00, 100.27it/s, loss=5.78e+4, metrics={'r2': 0.8489}]\n",
      "epoch 89: 100%|██████████| 1366/1366 [00:16<00:00, 83.53it/s, loss=5.78e+4, metrics={'r2': 0.8488}] \n",
      "epoch 90: 100%|██████████| 1366/1366 [00:13<00:00, 103.76it/s, loss=5.76e+4, metrics={'r2': 0.8498}]\n",
      "epoch 91: 100%|██████████| 1366/1366 [00:12<00:00, 107.15it/s, loss=5.77e+4, metrics={'r2': 0.8491}]\n",
      "epoch 92: 100%|██████████| 1366/1366 [00:12<00:00, 110.43it/s, loss=5.75e+4, metrics={'r2': 0.8505}]\n",
      "epoch 93: 100%|██████████| 1366/1366 [00:13<00:00, 99.61it/s, loss=5.74e+4, metrics={'r2': 0.8506}] \n",
      "epoch 94: 100%|██████████| 1366/1366 [00:14<00:00, 95.62it/s, loss=5.75e+4, metrics={'r2': 0.8501}] \n",
      "epoch 95: 100%|██████████| 1366/1366 [00:13<00:00, 102.14it/s, loss=5.75e+4, metrics={'r2': 0.8503}]\n",
      "epoch 96: 100%|██████████| 1366/1366 [00:13<00:00, 104.60it/s, loss=5.74e+4, metrics={'r2': 0.851}] \n",
      "epoch 97: 100%|██████████| 1366/1366 [00:14<00:00, 92.67it/s, loss=5.74e+4, metrics={'r2': 0.8505}] \n",
      "epoch 98: 100%|██████████| 1366/1366 [00:14<00:00, 96.53it/s, loss=5.74e+4, metrics={'r2': 0.8509}] \n",
      "epoch 99: 100%|██████████| 1366/1366 [00:12<00:00, 108.97it/s, loss=5.75e+4, metrics={'r2': 0.8503}]\n",
      "epoch 100: 100%|██████████| 1366/1366 [00:13<00:00, 104.54it/s, loss=5.73e+4, metrics={'r2': 0.8515}]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "continuous_cols = ['dist_to_nearest_stn', 'dist_to_dhoby', 'degree_centrality', 'eigenvector_centrality', 'remaining_lease_years', 'floor_area_sqm']\n",
    "cat_col_names = ['month', 'town', 'flat_model_type', 'storey_range']\n",
    "\n",
    "cat_embed_cols = [\n",
    "    (\"month\", len(train_df[\"month\"].unique())),\n",
    "    (\"town\", len(train_df[\"town\"].unique())),\n",
    "    (\"flat_model_type\", len(train_df[\"flat_model_type\"].unique())),\n",
    "    (\"storey_range\", len(train_df[\"storey_range\"].unique())),\n",
    "]\n",
    "\n",
    "tab_preprocessor = TabPreprocessor(\n",
    "    cat_embed_cols=cat_embed_cols, continuous_cols=continuous_cols\n",
    ")\n",
    "\n",
    "X_tab = tab_preprocessor.fit_transform(train_df)\n",
    "\n",
    "model = TabMlp(mlp_hidden_dims=[200,100], column_idx=tab_preprocessor.column_idx, cat_embed_input=tab_preprocessor.cat_embed_input, continuous_cols = continuous_cols)\n",
    "model = WideDeep(deeptabular=model)\n",
    "trainer = Trainer(model, objective='rmse', metrics=[R2Score], device = 'cpu', num_workers=0)\n",
    "# trainer.fit(X_wide=model, X_tab=None, target=df['resale_price'], n_epochs=100, batch_size=64)\n",
    "trainer.fit(X_tab=X_tab, target=np.array(train_df['resale_price']), n_epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V46s-MdM0y5c"
   },
   "source": [
    ">Report the test RMSE and the test R2 value that you obtained.\n",
    "\n",
    "RSME Score: 106068\n",
    "R2 Score: 0.607"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KAhAgvMC07g6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 1128/1128 [00:03<00:00, 365.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSME Score: 106067.97369493851\n",
      "R2 Score: 0.6069325560507035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_test_tab = tab_preprocessor.transform(test_df)\n",
    "pred = trainer.predict(X_tab=X_test_tab, batch_size=64)\n",
    "rsme = pow(mean_squared_error(test_df['resale_price'], pred), 0.5)\n",
    "r2 = R2Score()\n",
    "r2_score = r2(pred, test_df['resale_price'])\n",
    "\n",
    "print(\"RSME Score:\", rsme)\n",
    "print(\"R2 Score:\", r2_score)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
